<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>COVID-19 vaccine tweet sentiment analysis with fastai - part 1 | Tom Whelan‚Äôs blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="COVID-19 vaccine tweet sentiment analysis with fastai - part 1" />
<meta name="author" content="Tom Whelan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is part one of a two-part NLP series where we carry out sentiment analysis on COVID-19 vaccine tweets. In this part we follow the ULMFiT approach with fastai to create a Twitter language model, then use this to fine-tune a tweet sentiment classification model." />
<meta property="og:description" content="This is part one of a two-part NLP series where we carry out sentiment analysis on COVID-19 vaccine tweets. In this part we follow the ULMFiT approach with fastai to create a Twitter language model, then use this to fine-tune a tweet sentiment classification model." />
<link rel="canonical" href="https://www.thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.html" />
<meta property="og:url" content="https://www.thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.html" />
<meta property="og:site_name" content="Tom Whelan‚Äôs blog" />
<meta property="og:image" content="https://www.thomaswhelan.com/images/twitter-sentiment-analysis.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-17T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://www.thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.html","@type":"BlogPosting","headline":"COVID-19 vaccine tweet sentiment analysis with fastai - part 1","dateModified":"2021-03-17T00:00:00-05:00","datePublished":"2021-03-17T00:00:00-05:00","image":"https://www.thomaswhelan.com/images/twitter-sentiment-analysis.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.html"},"author":{"@type":"Person","name":"Tom Whelan"},"description":"This is part one of a two-part NLP series where we carry out sentiment analysis on COVID-19 vaccine tweets. In this part we follow the ULMFiT approach with fastai to create a Twitter language model, then use this to fine-tune a tweet sentiment classification model.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.thomaswhelan.com/feed.xml" title="Tom Whelan's blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-DW88MZC1TE','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Tom Whelan&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">üë§ About Me</a><a class="page-link" href="/search/">üîç Search</a><a class="page-link" href="/categories/">üè∑Ô∏è Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">COVID-19 vaccine tweet sentiment analysis with fastai - part 1</h1><p class="page-description">This is part one of a two-part NLP series where we carry out sentiment analysis on COVID-19 vaccine tweets. In this part we follow the ULMFiT approach with fastai to create a Twitter language model, then use this to fine-tune a tweet sentiment classification model.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-17T00:00:00-05:00" itemprop="datePublished">
        Mar 17, 2021
      </time>‚Ä¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Tom Whelan</span></span>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      20 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#NLP">NLP</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#sentiment analysis">sentiment analysis</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#PyTorch">PyTorch</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/twhelan22/blog/tree/master/_notebooks/2021-03-17-covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/twhelan22/blog/blob/master/_notebooks/2021-03-17-covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h1"><a href="#Transfer-learning-in-NLP---the-ULMFiT-approach">Transfer learning in NLP - the ULMFiT approach </a></li>
<li class="toc-entry toc-h1"><a href="#Data-preparation">Data preparation </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Getting-the-data-from-Kaggle">Getting the data from Kaggle </a></li>
<li class="toc-entry toc-h2"><a href="#Loading-and-cleaning-the-data">Loading and cleaning the data </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Training-a-language-model">Training a language model </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Fine-tuning-the-language-model">Fine-tuning the language model </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Training-a-sentiment-classifier">Training a sentiment classifier </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Fine-tuning-the-classifier">Fine-tuning the classifier </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Classifying-unlabelled-tweets">Classifying unlabelled tweets </a></li>
<li class="toc-entry toc-h1"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-17-covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p>In this post we will create a model to perform sentiment analysis on tweets about COVID-19 vaccines using the <code>fastai</code> library. I will provide a brief overview of the process here, but a much more in-depth explanation of NLP with <code>fastai</code> can be found in <a href="https://course.fast.ai/videos/?lesson=8">lesson 8</a> of the <code>fastai</code> course. In <a href="https://thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/visualisation/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-2.html">part 2</a> we will use the model for analysis, looking at changes in tweet sentiment over time and how that relates to the progress of vaccination in different countries.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Transfer-learning-in-NLP---the-ULMFiT-approach">
<a class="anchor" href="#Transfer-learning-in-NLP---the-ULMFiT-approach" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transfer learning in NLP - the ULMFiT approach<a class="anchor-link" href="#Transfer-learning-in-NLP---the-ULMFiT-approach"> </a>
</h1>
<p>We will be making use of <em>transfer learning</em> to help us create a model to analyse tweet sentiment. The idea behind transfer learning is that neural networks learn information that generalises to new problems, <a href="https://arxiv.org/pdf/1311.2901.pdf">particularly the early layers of the network</a>. In computer vision, for example, we can take a model that was trained on the ImageNet dataset to recognise different features of images such as circles, then apply that to a smaller dataset and <em>fine-tune</em> the model to be more suited to a specific task (e.g. classifying images as cats or dogs). This technique allows us to train neural networks much faster and with far less data than we would otherwise need.</p>
<p>In 2018 <a href="https://arxiv.org/abs/1801.06146">a paper</a> introduced a transfer learning technique for NLP called 'Universal Language Model Fine-Tuning' (ULMFiT). The approach is as follows:</p>
<ol>
<li>Train a <em>language model</em> to predict the next word in a sentence. This step is already done for us; with <code>fastai</code> we can download a model that has been pre-trained for this task on millions of Wikipedia articles. A good language model already knows a lot about how language works in general - for  instance, given the sentence 'Tokyo is the capital of', the model might predict 'Japan' as the next word. In this case the model understands that Tokyo is closely related to Japan and that 'capital' refers to 'city' here instead of 'upper-case' or 'money'.</li>
<li>Fine-tune the language model to a more specific task. The pre-trained language model is good at understanding Wikipedia English, but Twitter English is a bit different. We can take the information the Wikipedia model has learned and apply that to a Twitter dataset to get a Twitter language model that is good at predicting the next word in a tweet.</li>
<li>Fine-tune a <em>classification model</em> to identify sentiment using the pre-trained language model. The idea here is that since our language model already knows a lot about Twitter English, it's not a huge leap from there to train a classifier that understands that 'love' refers to positive sentiment and 'hate' refers to negative sentiment. If we tried to train a classifier without using a pre-trained model it would have to learn the whole language from scratch first, which would be very difficult and time consuming.</li>
</ol>
<p><img src="https://github.com/twhelan22/blog/blob/master/images/ulmfit_process.png?raw=true" alt="" title="Credit: https://course.fast.ai/"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This notebook will walk through steps 2 and 3 with <code>fastai</code>. We will then apply the model to unlabelled COVID-19 vaccine tweets and save the results for analysis in <a href="https://thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/visualisation/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-2.html">part 2</a>.
</p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z"></path></svg>
    <strong>Important: </strong>You will need a GPU to train models with <code>fastai</code>, but fortunately for us Google Colab provides us with access to one for free! To use it, select ‚ÄôRuntime‚Äô from the menu at the top of the notebook, then ‚ÄôChange runtime type‚Äô, and ensure your hardware accelerator is set to ‚ÄôGPU‚Äô before continuing!
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Data-preparation">
<a class="anchor" href="#Data-preparation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data preparation<a class="anchor-link" href="#Data-preparation"> </a>
</h1>
<p>This is a write-up of a submission I made for several <a href="https://www.kaggle.com/gpreda/all-covid19-vaccines-tweets/tasks">Kaggle tasks</a>. The tasks are still open and accepting new entries at the time of writing if you want to enter as well! On Kaggle the data is already readily available when using their notebook servers; however, we are using Google Colab today, so we will need to access the <a href="https://www.kaggle.com/docs/api">Kaggle API</a> to download the data.
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>Kaggle also have free GPU credits if you prefer to work on their notebook servers instead.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Getting-the-data-from-Kaggle">
<a class="anchor" href="#Getting-the-data-from-Kaggle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting the data from Kaggle<a class="anchor-link" href="#Getting-the-data-from-Kaggle"> </a>
</h2>
<p>The first step is to create an API token. To do this, the steps are as follows:</p>
<ol>
<li>Go to 'Account' on Kaggle and scroll down to the 'API' section.</li>
<li>Expire all current API tokens by clicking 'Expire API Token'.</li>
<li>Click 'Create New API Token', which will automatically download a file called <code>kaggle.json</code>.</li>
<li>Upload the <code>kaggle.json</code> file using the file uploader widget below.</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># See https://neptune.ai/blog/google-colab-dealing-with-files for more tips on working with files in Colab</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>
<span class="n">uploaded</span> <span class="o">=</span> <span class="n">files</span><span class="o">.</span><span class="n">upload</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

     <input type="file" id="files-a043b98a-3192-4d9e-87b1-bc6df9643f02" name="files[]" multiple disabled style="border:none">
     <output id="result-a043b98a-3192-4d9e-87b1-bc6df9643f02">
      Upload widget is only available when the cell has been executed in the
      current browser session. Please rerun this cell to enable.
      </output>
      <script src="/nbextensions/google.colab/files.js"></script> 
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving kaggle.json to kaggle.json
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we need to install the Kaggle API.
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>The API is already preinstalled in Google Colab, but sometimes it‚Äôs an outdated version, so it‚Äôs best to upgrade it in case.
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip uninstall -q -y kaggle
<span class="o">!</span>pip install -q --upgrade pip
<span class="o">!</span>pip install -q --upgrade kaggle
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.5MB 5.4MB/s 
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 2.8 MB/s 
  Building wheel for kaggle (setup.py) ... done
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <a href="https://github.com/Kaggle/kaggle-api">API docs</a> tell us that we need to ensure <code>kaggle.json</code> is in the location <code>~/.kaggle/kaggle.json</code>, so let's make the directory and move the file.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># https://www.machinelearningmindset.com/kaggle-dataset-in-google-colab/</span>
<span class="o">!</span>mkdir -p ~/.kaggle
<span class="o">!</span>cp kaggle.json ~/.kaggle/
<span class="c1"># Check the file in its new directory</span>
<span class="o">!</span>ls /root/.kaggle/
<span class="c1"># Check the file permission</span>
<span class="o">!</span>ls -l ~/.kaggle/kaggle.json
<span class="c1">#Change the file permission</span>
<span class="c1"># chmod 600 file ‚Äì owner can read and write</span>
<span class="c1"># chmod 700 file ‚Äì owner can read, write and execute</span>
<span class="o">!</span>chmod <span class="m">600</span> ~/.kaggle/kaggle.json
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>kaggle.json
-rw-r--r-- 1 root root 63 Mar 17 15:35 /root/.kaggle/kaggle.json
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can download the data using <code>!kaggle dataset download -d username-of-dataset-creator/name-of-dataset</code>.
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>There is also an API download command on the dataset page that you can copy/paste instead.
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># We will be using two datasets for this part, as well as a third dataset for part 2</span>
<span class="c1"># To save time in part 2 I'm going to download them all now and save locally</span>
<span class="o">!</span>kaggle datasets download -d gpreda/all-covid19-vaccines-tweets
<span class="o">!</span>kaggle datasets download -d maxjon/complete-tweet-sentiment-extraction-data
<span class="o">!</span>kaggle datasets download -d gpreda/covid-world-vaccination-progress
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Downloading all-covid19-vaccines-tweets.zip to /content
  0% 0.00/4.76M [00:00&lt;?, ?B/s]
100% 4.76M/4.76M [00:00&lt;00:00, 156MB/s]
Downloading complete-tweet-sentiment-extraction-data.zip to /content
  0% 0.00/2.58M [00:00&lt;?, ?B/s]
100% 2.58M/2.58M [00:00&lt;00:00, 148MB/s]
Downloading covid-world-vaccination-progress.zip to /content
  0% 0.00/146k [00:00&lt;?, ?B/s]
100% 146k/146k [00:00&lt;00:00, 61.1MB/s]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The files will be downloaded in <code>.zip</code> format, so let's unzip them.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># To unzip you can use the following:</span>
<span class="c1">#!mkdir folder_name</span>
<span class="c1">#!unzip anyfile.zip -d folder_name</span>

<span class="c1"># Or unzip all</span>
<span class="o">!</span>unzip -q <span class="se">\*</span>.zip
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
3 archives were successfully processed.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loading-and-cleaning-the-data">
<a class="anchor" href="#Loading-and-cleaning-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loading and cleaning the data<a class="anchor-link" href="#Loading-and-cleaning-the-data"> </a>
</h2>
<p>As with <code>kaggle</code>, an older version of <code>fastai</code> is preinstalled in Colab, so we will need to upgrade it first.
</p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z"></path></svg>
    <strong>Important: </strong>Make a note of the <code>fastai</code> version you are using, since any models you create and save will need to be run using the same version later.
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> <span class="o">[</span> -e /content <span class="o">]</span> <span class="o">&amp;&amp;</span> pip install -Uqq fastai  # upgrade fastai on colab
<span class="kn">import</span> <span class="nn">fastai</span><span class="p">;</span> <span class="n">fastai</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 193 kB 4.1 MB/s 
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 776.8 MB 18 kB/s 
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.8 MB 23 kB/s 
     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53 kB 1.4 MB/s 
<span class="ansi-red-fg">ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchtext 0.9.0 requires torch==1.8.0, but you have torch 1.7.1 which is incompatible.</span>
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>'2.2.7'</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's import <code>fastai</code>'s <code>text</code> module and take a look at our data.
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>If you use <code>import *</code>, useful libraries like <code>pandas</code> and <code>numpy</code> will also be imported at the same time!
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.text.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vax_tweets</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'vaccination_all_tweets.csv'</span><span class="p">)</span>
<span class="n">vax_tweets</span><span class="p">[[</span><span class="s1">'date'</span><span class="p">,</span> <span class="s1">'text'</span><span class="p">,</span> <span class="s1">'hashtags'</span><span class="p">,</span> <span class="s1">'user_followers'</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>text</th>
      <th>hashtags</th>
      <th>user_followers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-12-20 06:06:44</td>
      <td>Same folks said daikon paste could treat a cytokine storm #PfizerBioNTech https://t.co/xeHhIMg1kF</td>
      <td>['PfizerBioNTech']</td>
      <td>405</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2020-12-13 16:27:13</td>
      <td>While the world has been on the wrong side of history this year, hopefully, the biggest vaccination effort we've ev‚Ä¶ https://t.co/dlCHrZjkhm</td>
      <td>NaN</td>
      <td>834</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2020-12-12 20:33:45</td>
      <td>#coronavirus #SputnikV #AstraZeneca #PfizerBioNTech #Moderna #Covid_19 Russian vaccine is created to last 2-4 years‚Ä¶ https://t.co/ieYlCKBr8P</td>
      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'PfizerBioNTech', 'Moderna', 'Covid_19']</td>
      <td>10</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020-12-12 20:23:59</td>
      <td>Facts are immutable, Senator, even when you're not ethically sturdy enough to acknowledge them. (1) You were born i‚Ä¶ https://t.co/jqgV18kch4</td>
      <td>NaN</td>
      <td>49165</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2020-12-12 20:17:19</td>
      <td>Explain to me again why we need a vaccine @BorisJohnson @MattHancock #whereareallthesickpeople #PfizerBioNTech‚Ä¶ https://t.co/KxbSRoBEHq</td>
      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>
      <td>152</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We could use the <code>text</code> column of this dataset to train a Twitter language model, but since our end goal is sentiment analysis we will need to find another dataset that also contains sentiment labels to train our classifier. Let's use <a href="https://www.kaggle.com/maxjon/complete-tweet-sentiment-extraction-data">'Complete Tweet Sentiment Extraction Data'</a>, which contains 40,000 tweets labelled as either negative, neutral or positive sentiment. For more accurate results you could use the <a href="https://www.kaggle.com/kazanova/sentiment140">'sentiment140'</a> dataset instead, which contains 1.6m tweets labelled as either positive or negative.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tweets</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'tweet_dataset.csv'</span><span class="p">)</span>
<span class="n">tweets</span><span class="p">[[</span><span class="s1">'old_text'</span><span class="p">,</span> <span class="s1">'new_sentiment'</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>old_text</th>
      <th>new_sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Funeral ceremony...gloomy friday...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>3</th>
      <td>wants to hang out with friends SOON!</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>4</th>
      <td>@dannycastillo We want to trade with someone who has Houston tickets, but no one will.</td>
      <td>neutral</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For our language model, the only input we need is the tweet text. As we will see in a moment <code>fastai</code> can handle text preprocessing and tokenization for us, but it might be a good idea to remove things like twitter handles, urls, hashtags and emojis first. You could experiment with leaving these in for your own models and see how it affects the results. There are also some rows with blank tweets which need to be removed.</p>
<p>We ideally want the language model to learn not just about tweet language, but more specifically about vaccine tweet language. We can therefore use text from both datasets as input for the language model. For the classification model we need to remove all rows with missing sentiment, however.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">de_emojify</span><span class="p">(</span><span class="n">inputString</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">inputString</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">'ascii'</span><span class="p">,</span> <span class="s1">'ignore'</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">'ascii'</span><span class="p">)</span>

<span class="c1"># Code via https://www.kaggle.com/pawanbhandarkar/generate-smarter-word-clouds-with-log-likelihood</span>
<span class="k">def</span> <span class="nf">tweet_proc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">text_col</span><span class="o">=</span><span class="s1">'text'</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">'orig_text'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span>
    <span class="c1"># Remove twitter handles</span>
    <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">'@[^\s]+'</span><span class="p">,</span><span class="s1">''</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>
    <span class="c1"># Remove URLs</span>
    <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">"http\S+"</span><span class="p">,</span> <span class="s2">""</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
    <span class="c1"># Remove emojis</span>
    <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">de_emojify</span><span class="p">)</span>
    <span class="c1"># Remove hashtags</span>
    <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">'\B#\S+'</span><span class="p">,</span><span class="s1">''</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">text_col</span><span class="p">]</span><span class="o">!=</span><span class="s1">''</span><span class="p">]</span>

<span class="c1"># Clean the text data and combine the dfs</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="n">tweets</span><span class="p">[[</span><span class="s1">'old_text'</span><span class="p">,</span> <span class="s1">'new_sentiment'</span><span class="p">]]</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">'old_text'</span><span class="p">:</span><span class="s1">'text'</span><span class="p">,</span> <span class="s1">'new_sentiment'</span><span class="p">:</span><span class="s1">'sentiment'</span><span class="p">})</span>
<span class="n">vax_tweets</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="n">tweet_proc</span><span class="p">(</span><span class="n">tweets</span><span class="p">)</span>
<span class="n">vax_tweets</span> <span class="o">=</span> <span class="n">tweet_proc</span><span class="p">(</span><span class="n">vax_tweets</span><span class="p">)</span>
<span class="n">df_lm</span> <span class="o">=</span> <span class="n">tweets</span><span class="p">[[</span><span class="s1">'text'</span><span class="p">,</span> <span class="s1">'sentiment'</span><span class="p">]]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vax_tweets</span><span class="p">[[</span><span class="s1">'text'</span><span class="p">,</span> <span class="s1">'sentiment'</span><span class="p">]])</span>
<span class="n">df_clas</span> <span class="o">=</span> <span class="n">df_lm</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_lm</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_clas</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>70732 31323
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_clas</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Layin n bed with a headache  ughhhh...waitin on your call...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Funeral ceremony...gloomy friday...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>3</th>
      <td>wants to hang out with friends SOON!</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>4</th>
      <td>We want to trade with someone who has Houston tickets, but no one will.</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Re-pinging  why didn't you go to prom? BC my bf didn't like my friends</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-a-language-model">
<a class="anchor" href="#Training-a-language-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training a language model<a class="anchor-link" href="#Training-a-language-model"> </a>
</h1>
<p>To train our language model we can use self-supervised learning; we just need to give the model some text as an independent variable and <code>fastai</code> will automatically preprocess it and create a dependent variable for us. We can do this in one line of code using the <code>DataLoaders</code> class, which converts our input data into a <code>DataLoader</code> object that can be used as an input to a <code>fastai</code> <code>Learner</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls_lm</span> <span class="o">=</span> <span class="n">TextDataLoaders</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="n">df_lm</span><span class="p">,</span> <span class="n">text_col</span><span class="o">=</span><span class="s1">'text'</span><span class="p">,</span> <span class="n">is_lm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we told <code>fastai</code> that we are working with text data, which is contained in the <code>text</code> column of a <code>pandas</code> <code>DataFrame</code> called <code>df_lm</code>. We set <code>is_lm=True</code> since we want to train a language model, so <code>fastai</code> needs to label the input data for us. Finally, we told <code>fastai</code> to hold out a random 10% of our data for a validation set using <code>valid_pct=0.1</code>.</p>
<p>Let's take a look at the first two rows of the <code>DataLoader</code> using <code>show_batch</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls_lm</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>text_</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>xxbos xxup rip xxmaj big cup ‚Ä¶ i will miss you xxbos xxmaj morning do n't ask me why xxmaj i 'm up so early xxbos xxmaj swiss drugmaker to help make xxbos xxmaj roast was yummy , i think mum was impressed xxrep 3 ! xxbos tol xxrep 3 d you there was thunder ! ew now it 's all rainy xxup d : i 'm scared ! xxbos thanks xxmaj</td>
      <td>xxup rip xxmaj big cup ‚Ä¶ i will miss you xxbos xxmaj morning do n't ask me why xxmaj i 'm up so early xxbos xxmaj swiss drugmaker to help make xxbos xxmaj roast was yummy , i think mum was impressed xxrep 3 ! xxbos tol xxrep 3 d you there was thunder ! ew now it 's all rainy xxup d : i 'm scared ! xxbos thanks xxmaj xxunk</td>
    </tr>
    <tr>
      <th>1</th>
      <td>prime minister for \n xxmaj xxunk xxmaj xxunk . xxbos xxmaj thread : xxmaj as the rolls out the &amp; &amp; how s your state , ( district or xxunk xxbos xxmaj an hour of walking in hot weather = a satisfied but hurting xxmaj xxunk . xxmaj ow , blisters . xxbos awesome that s what i m xxunk the song paranoid . its stuck in my head ! and i</td>
      <td>minister for \n xxmaj xxunk xxmaj xxunk . xxbos xxmaj thread : xxmaj as the rolls out the &amp; &amp; how s your state , ( district or xxunk xxbos xxmaj an hour of walking in hot weather = a satisfied but hurting xxmaj xxunk . xxmaj ow , blisters . xxbos awesome that s what i m xxunk the song paranoid . its stuck in my head ! and i love</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have a new column, <code>text_</code>, which is <code>text</code> offset by one. This is the dependent variable <code>fastai</code> created for us. By default <code>fastai</code> uses <em>word tokenization</em>, which splits the text on spaces and punctuation marks and breaks up words like <em>can't</em> into two separate tokens. <code>fastai</code> also has some special tokens starting with 'xx' that are designed to make things easier for the model; for example <code>xxmaj</code> indicates that the next word begins with a capital letter and <code>xxunk</code> represents an unknown word that doesn't appear in the vocabulary very often. You could experiment with <em>subword tokenization</em> instead, which will split the text on commonly occuring groups of letters instead of spaces. This might help if you wanted to leave hashtags in since they often contain multiple words joined together with no spaces, e.g. #CovidVaccine. The <code>fastai</code> tokenization process is explained in much more detail <a href="https://youtu.be/WjnwWeGjZcM?t=626">here</a> for those interested.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-tuning-the-language-model">
<a class="anchor" href="#Fine-tuning-the-language-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fine-tuning the language model<a class="anchor-link" href="#Fine-tuning-the-language-model"> </a>
</h2>
<p>The next step is to create a language model using <code>language_model_learner</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">language_model_learner</span><span class="p">(</span><span class="n">dls_lm</span><span class="p">,</span> <span class="n">AWD_LSTM</span><span class="p">,</span> <span class="n">drop_mult</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> 
                               <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">Perplexity</span><span class="p">()])</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we passed <code>language_model_learner</code> our <code>DataLoaders</code>, <code>dls_lm</code>, and the pre-trained <a href="https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn">RNN</a> model, <a href="https://docs.fast.ai/text.models.awdlstm.html"><em>AWD_LSTM</em></a>, which is built into <code>fastai</code>. <code>drop_mult</code> is a multiplier applied to all <a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/">dropouts</a> in the AWD_LSTM model to reduce overfitting. For example, by default <code>fastai</code>'s AWD_LSTM applies <code>EmbeddingDropout</code> with 10% probability (at the time of writing), but we told <code>fastai</code> that we want to reduce that to 3%. The <code>metrics</code> we want to track are <em>perplexity</em>, which is the exponential of the loss (in this case cross entropy loss), and <em>accuracy</em>, which tells us how often our model predicts the next word correctly. We can also train with fp16 to use less memory and speed up the training process.</p>
<p>We can find a good learning rate for training using <code>lr_find</code> and use that to fit our model.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.04365158379077912, lr_steep=0.02754228748381138)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5ycZbn/8c+1vWRLyu6mkgIhhUBIQTrCoSMCAoqINBEOykE9Hj3g+dk9KrZzBBFCKKEcVDQigkAEEQyEuiGhpRFSyIZks9k62V6u3x8zCcu6SXbJPjPP7Hzfr9e82HnqN5Mw197P/Tz3be6OiIikrrREBxARkcRSIRARSXEqBCIiKU6FQEQkxakQiIikOBUCEZEUl5HoAP01YsQInzBhQqJjiIgklaVLl25395Le1iVdIZgwYQLl5eWJjiEiklTMbOPu1unSkIhIilMhEBFJcSoEIiIpToVARCTFqRCIiKQ4FQIRkRSnQiAikgSeXFHJ2m2RQI6tQiAiEnLuzhfvX8ofX90cyPFVCEREQq61o4v2TqcgJ5hngFUIRERCrqGlHYCCnMxAjq9CICIScg3NHQAUqkUgIpKaIrEWQaFaBCIiqSnSEm0RqI9ARCRFvV8I1CIQEUlJ73cWJ1mLwMymmNnybq8GM/tKj23MzG4ys7Vm9rqZzQ4qj4hIstrVR5AbTIsgsIlp3H01cCiAmaUDm4E/9djsdGBy7HU4cGvsvyIiEhNp6SDNID8rPZDjx+vS0InAO+7ec4acs4F7PepFoNjMRsUpk4hIUoi0dDAkOwMzC+T48SoEnwZ+28vyMcCmbu8rYss+wMyuMrNyMyuvqqoKKKKISDg1NLcHdlkI4lAIzCwLOAv4w4c9hrvPd/e57j63pKTXuZdFRAathpaOwO4Ygvi0CE4HXnX3yl7WbQbGdXs/NrZMRERiIi3tgd0xBPEpBBfS+2UhgIeBS2J3Dx0B1Lv7ljhkEhFJGpGWjsCGl4AA7xoCMLN84GTgX7stuxrA3ecBjwFnAGuBJuDyIPOIiCSjhpZ2puYUBHb8QAuBuzcCw3ssm9ftZweuCTKDiEiyi7R0JP2lIRER+ZDcnR2tyd9ZLCIiH1JTWyedXcFNSgMqBCIiodYQ8PASoEIgIhJqQQ9BDSoEIiKhFgl4mkpQIRARCbUGtQhERFJbQ3Ow01SCCoGISKjt7CMI8sliFQIRkRALeppKUCEQEQm1SEs7GWlGTmZwX9cqBCIiIdbQEp2LIKhJaUCFQEQk1IIeZwhUCEREQk2FQEQkxUVa2inIDq6jGFQIRERCraG5g8JctQhERFJWdJpKtQhERFKW+ghERFJYZ5cTCXhSGlAhEBEJrR2twQ8vASoEIiKhtXMI6iAHnAMVAhGR0IrHpDSgQiAiElo7h6BO6j4CMys2s4VmtsrMVprZkT3WF5nZI2b2mpm9ZWaXB5lHRCSZ7BqCOuDnCII9OtwILHL3880sC8jrsf4aYIW7f9zMSoDVZna/u7cFnEtEJPQirfFpEQRWCMysCDgOuAwg9uXe8wvegQKLDqs3BKgBOoLKJCKSTAZDH8FEoApYYGbLzOwOM8vvsc3NwDTgPeAN4Mvu3tXzQGZ2lZmVm1l5VVVVgJFFRMLj/T6C5C0EGcBs4FZ3nwU0Atf32OZUYDkwGjgUuNnMCnseyN3nu/tcd59bUlISYGQRkfCItHSQnZFGdkZ6oOcJshBUABXu/lLs/UKihaG7y4EHPWotsB6YGmAmEZGk0dAS/FPFEGAhcPetwCYzmxJbdCKwosdm78aWY2ZlwBRgXVCZRESSSaSlPfCniiH4u4auBe6P3TG0DrjczK4GcPd5wA+Au83sDcCA69x9e8CZRESSQkMcBpyDgAuBuy8H5vZYPK/b+veAU4LMICKSrCKx+YqDpieLRURCKh5DUIMKgYhIaMVjmkpQIRARCa2GZrUIRERSVntnF83tneojEBFJVTviNLwEqBCIiITS++MMqUUgIpKSGlriM84QqBCIiIRSXVO0EAzNywr8XCoEIiIhVNMUHbV/aJ4uDYmIpKS6nYUgXy0CEZGUVNMYLQTFun1URCQ11Ta2UZiTQUZ68F/TKgQiIiFU29TOsDhcFgIVAhGRUKptaotL/wCoEIiIhFJNY1tcbh0FFQIRkVCqa2pXIRARSWXRFkHwdwyBCoGISOi0tHfS3N6pPgIRkVRVG3uYTHcNiYikqJ0Pk+nSkIhIiorngHMQcCEws2IzW2hmq8xspZkd2cs2x5vZcjN7y8z+EWQeEZFksLNFEK9LQ0EPdH0jsMjdzzezLCCv+0ozKwZuAU5z93fNrDTgPCIiobezj6A4Ti2CwAqBmRUBxwGXAbh7G9DWY7PPAA+6+7uxbbYFlUdEJFnUNkYvDRUPgj6CiUAVsMDMlpnZHWaW32ObA4GhZvaMmS01s0sCzCMikhRqm9ooyMkgMw4DzkGwhSADmA3c6u6zgEbg+l62mQN8DDgV+JaZHdjzQGZ2lZmVm1l5VVVVgJFFRBKvprEtbv0DEGwhqAAq3P2l2PuFRAtDz23+6u6N7r4dWAzM7Hkgd5/v7nPdfW5JSUmAkUVEEq+2KX7jDEGAhcDdtwKbzGxKbNGJwIoem/0ZOMbMMswsDzgcWBlUJhGRZBAtBPHpH4Dg7xq6Frg/dsfQOuByM7sawN3nuftKM1sEvA50AXe4+5sBZxIRCbXaxnYOLCuI2/kCLQTuvhyY22PxvB7b/Az4WZA5RESSSW1TG8MGw6UhERHpv5b2Tpra4jfgHKgQiIiEys6HyQZFZ7GIiPTfzofJ4tlZrEIgIhIiu1oEujQkIpKa4j3gHKgQiIiESt2uAed0aUhEJCXVNMZ3LgJQIRARCZV4DzgHKgQiIqES73GGQIVARCRUahrb4nrHEKgQiIiESl1TO8Pi2FEMKgQiIqFS06hLQyIiKa22KaSXhsws38zSYj8faGZnmVl82y4iIoPcrgHnQnppaDGQY2ZjgCeAi4G7gwolIpKK6ppizxCEsUUAmLs3AecCt7j7J4GDgoslIpJ6dg0vEdI+AjOzI4GLgEdjy9KDiSQikpreH14inIXgK8A3gD+5+1tmNgl4OrhYIiKpp6Yp/gPOQR+nqnT3fwD/AIh1Gm939y8FGUxEJNXUNu4cgjqEncVm9hszKzSzfOBNYIWZfT3YaCIiqaVqRxtm8R1wDvp+aWi6uzcA5wCPAxOJ3jkkIiIDpCrSwvD8rLgOOAd9LwSZsecGzgEedvd2wIOLJSKSeiobWiktyIn7eftaCG4DNgD5wGIzGw807G0nMys2s4VmtsrMVsbuPOptu8PMrMPMzu9rcBGRwWZbpIXSwuy4n7dPhcDdb3L3Me5+hkdtBE7ow643AovcfSowE1jZcwMzSwd+QvRBNRGRlLWtoZWysLYIzKzIzP7HzMpjr18QbR3scR/gOOBOAHdvc/e6Xja9FvgjsK1/0UVEBo/OLmf7jtbwtgiAu4AI8KnYqwFYsJd9JgJVwAIzW2Zmd8TuOtolNmTFJ4Bb93QgM7tqZxGqqqrqY2QRkeRRvaOVLofSgvAWgv3d/Tvuvi72+h4waS/7ZACzgVvdfRbQCFzfY5tfAte5e9eeDuTu8919rrvPLSkp6WNkEZHkUdnQCkBpYUgvDQHNZnbMzjdmdjTQvJd9KoAKd38p9n4h0cLQ3Vzgd2a2ATgfuMXMzuljJhGRQWNbpAVITIugT08WA1cD98au+wPUApfuaQd332pmm8xsiruvBk4EVvTYZuLOn83sbuAv7v5QX8OLiAwWO1sEZQloEfR1iInXgJlmVhh732BmXwFe38uu1wL3m1kWsA643Myujh1j3oePLSIyuOxsEYwYEt4WARAtAN3efpXoNf49bb+c6OWf7notAO5+WX+yiIgMJtsirQzPzyIrI/4TR+7LGW3AUsTB5rpmbnh8FR2de+yXFhFJiG0NLZQkoH8A9q0QJNUQE29urmfeP97hD0srEh1FROSfbIu0JqR/APZSCMwsYmYNvbwiwOg4ZRwQp0wvY/Z+xfzvk2tobutMdBwRkQ+obGhJyB1DsJdC4O4F7l7Yy6vA3fvVv5BoZsb1p09jW6SVu5asT3QcEZFdok8Vt4WzRTDYfGTiME6aVsq8Z97ZNTeoiEii1TS20dnlCRleAlKsEAD852lTaWzr4NdPr010FBERIHpZCBLzMBmkYCE4sKyA8+eM5b4XNvJ6RW9j4ImIxFdVJHHDS0AKFgKAfz/5QPKy0znr5iVcvuBlXlpXjXtS3QQlIoNIolsESdXhO1BGFeXyzNeO594XNnL38xu4YP6LnDClhJ99cuZun+p7amUlP/jLCpraOsnOTCMnI51zZ4/l6o9OwiypHqkQkZDZFmsRJONzBEmtOC+LL504mSXX/Qvf/Ng0lrxTzek3PsuStds/sF1rRyfff2QFV9xTTk5mOv8ytZTDxg9jaH4WP1m0iv/60xt6SE1E9kllQwtD8zLJzkhPyPlTskXQXW5WOp8/dhJHHzCCa3+7jM/e+RIfO3gUxXmZZKan8fL6Gt56r4HLjprA9adPJScz+hfl7vziiTXc/PRatjW08qvPzCIvK+U/ThH5EBL5MBmoEOwybVQhD//b0fzosZU8uaKSjk6nrbOLguwM5l88h1MOGvmB7c2Mr506hbKiHL7z5ze58t5y/u+Kw3WZSET6bVukNWGXhUCF4APysjL473MO5r/PObjP+1x8xHgAvvXQmzz82nucfeiYoOKJyCC1raGFyaUjEnb+lO0jGEgXfWQ/Dh5TxI8fW0Vja0ei44hIEunqcqoirQm7YwhUCAZEWprx3bOms7WhhVue2bcH1RpbO1j05haeX7ud9dsbaWnXuEgig1lNUxsdXZ7QQqBLQwNkzvhhfGLWGG5fvJ5PzR3H+OH5/dq/vqmde17YwIIl66ltav/AupKCbMYPy2O/4XnMGF3EydPLGDcsbwDTi0iibEvgzGQ7qRAMoOtPn8pf39rK9x5ZwS0Xzd51h1Fvurqct7ft4OX11by0voZnVlexo7WDE6eW8rljJmIGW+paeK+umU21TWysbuL5tdU8+Opmvv+XFUwdWcDxU0qZOrKAyWVD2L9kyB7Plwibapp4ckUlyzfV0ekODmYwsjCH/YbnMW5YHmOLcxlVnMuQ7IH5p7ijtYMV7zWwsbqR8cPzmTKygKLczAE5dleX09jWQV1TO1sbWtha30KkpYORRdmMHZrH6OJc8rPSdcOA9EvlzrmKEzTOEKgQDKiywhy+ctJkfvTYKo748VNccNg4Pn3Yfrg779W1UFHbxKqtEVa818CKLQ3siPUnjCzM4bQZI/nc0ROZPrpwj+fYsL2Rv62s5IkVldzx7Do6uqJPRKcZjBuWx/4lQzigdAiz9yvmiEnDKc7LGtA/Y31zO6++W0v5hhre2NzAkOx0RhbmMrIoG3eoa26nrqmN1zbVs2JLdEK7McW5ZGemYUCXw5MrKmnt+OCzFwU5GZQV5jAsP4theVnkZqXT0NxOXXM7kZZ20szIzkgjMz1t1wN9OZnpOE5TWyfNbZ1si7SyobqRng+Jjy7KYfiQbPKy0snPzqA4L5PSgpxdd2ms3RZhTeUONtU04bHPMs2Mzi6ny53OLqelvYvmPlymy0w3CnIyKcjJYExxLgeWFXBA6RBGDMmipb2LlvZOWju66HLflbMgJ4Oi3EyK87IYVZTD6OJc0tNUTFJFVaxFUFqQuBaBJdvQCnPnzvXy8vJEx9gtd+eFddXc98JGnlhRSWfXBz/f3Mx0po8u5KDRhRwytpjDJw5j7NDcD/VbZFtHFxuqG1lTGeHtyh2srdrBO9t2sG57I20dXZjB9FGFHFhWQEFOBgU5GYwfls9pB4+kMOf935K3NbTw5MpKMtPTKCvMYWRhDu2dXVRFWtkWaWFTTTOrKyOsqYzwbk0T7pCRZhxYVkBrRydb6ltois3xkJluFOVmMWF4HqccVMbJ00cyccQHL5O5RzvH3q1pYnNdM1vqoy2fqkgrNY1t1Da10dTWGftyzKQgO5NOd9o7u2jr6KK1I/qF2tLeSZoZuVnRojAsL4vpowuZMaaQ8cPzebc6WnjXVEaoa2qjsbWTHa0d1DW1UbWjlfbO6N/N0LxMJpcVMHF4Pmlphnu0AKSZkZZmpJuRk5lGXlYGQ7KjX9plRdHPaUhOBlvrW9hc18x7dc3UxwpXQ3MHG2uaWFsZobGf819kphtjh+YxfXQhJ08r44QppRTlDUyrRsLn5r+/zc+fWMOqH5wWaKvezJa6e8+pg6PrVAiCs6W+mSfeqmRIdgZjhuYypjg3Lr/ttXd28XpFHc+vreb5d6qpqGsi0tJBpKWDzi4nJzONM2aM4oj9h/PEW1t5enXVPxWs7tLTjEkj8jlwZAFTywqYM2Eoh44r3vUAnbsTae0g3Yy8JLk00tXl1De30+nO8PyswDK7O+/Vt1Df1E5OZho5melkZ6RFi4wZjhNp6aC+uZ3apjY21zazsaaJjdWNvLy+lu07WslIM2aPH8ohY4qYEXvtX5KfFJ+z7N23HnqTR15/j+XfPiXQ86gQCBD9Unpjcz0PvLKJh5e/R6S1g9KCbM6bM5ZzZ40hJzN917XvzHSjpCCH0oJsygpzEjKhdqrr6nKWV9Tx5IpKnl+7nVVbI7suqU0uHcJ5c8byiVljEtrJKPvuynvL2VjdyBP//tFAz5OwQmBmxcAdwAyicxx/zt1f6Lb+IuA6wIAI8AV3f21Px1QhGBjNbZ28vS3C9FGFZKTrSz4ZdHR28U5VIy9vqOFPr1bw6rt1pBkcO7mE8+eM5eTpZaG7YUD27rRfLmbs0DzuuLTX7+gBs6dCEHRn8Y3AInc/38yygJ73PK4HPurutWZ2OjAfODzgTEJ0jKVDxhYnOob0Q0Z6GlNGFjBlZAEXHzGedVU7ePDVzTz4agXX/nYZhTkZnDdnLFcdN4lRRbmJjit94O5sqG7kmAMS91QxBFgIzKwIOA64DMDd24APzA/p7s93e/siMDaoPCKDzaSSIXzt1Cl89eQDeWFdNb8v38R9L2zk/17cyHmzx3L1R/dnwoj+Pc8i8bUt0kpLexfjE/z3FGSLYCJQBSwws5nAUuDL7t64m+2vAB4PMI/IoJSWZhx9wAiOPmAEXz91Crf9Yx0PlG/igfJNnDK9jCuPncSc8UPVuRxCG7ZHvw4nDE/sA6JBXhzOAGYDt7r7LKARuL63Dc3sBKKF4LrdrL/KzMrNrLyqqiqovCJJb+zQPH5wzgye+88T+OLx+/PiuhrOn/cCn7jleZ5aWamZ+EJmY3UTABP6ORLBQAuyEFQAFe7+Uuz9QqKF4QPM7BCiHcpnu3t1bwdy9/nuPtfd55aUlAQWWGSwKC3M4eunTuWFb/wLPzj7IKobW7ninnLOunkJf1uhghAWG6obyUw3RhUl9s6vwAqBu28FNpnZlNiiE4EV3bcxs/2AB4GL3X1NUFlEUlVeVgYXHzmBv//H8fz0/EOob27n8/eW8+n5L7KmMpLoeClvQ3Uj44bmJfzOvaDPfi1wv5m9DhwK/MjMrjazq2Prvw0MB24xs+VmpvtCRQKQmZ7Gp+aO46n/+Cg//MQMVldGOOPGZ/nhoyt2DXUi8bdhexPjE9w/AAHfPuruy4Ge963O67b+88Dng8wgIu/LTE/josPHc/qMUfx00Spuf3Y9j72xlV98aiZHTBqe6Hgpxd3ZWN3IRyYOS3QUzUcgkoqG5Wdxw3mH8McvHElmunHh7S/yo8dWav6LONq+o43Gts6E3zEEKgQiKW3O+GE8+qVjufAj+zF/8TrOvnkJK95rSHSslLCxOnrraKKfIQAVApGUl5+dwY8+cTALLjuMmqY2zv71c8z7xzt7HIhQ9t2GkNw6CioEIhJzwtRS/vqV4zhpWhk3PL6KC+e/yLaGlkTHGrQ2VjeSnmaMKU78cCAqBCKyy7D8LG65aDa/+ORM3thczzm/XsLKLbpUFIQN1U2MKc4Nxci+iU8gIqFiZpw3Zyx/uPpIuhzOv/V5/r6qMtGxBp3odKqJ7ygGFQIR2Y0ZY4r4878dzaSSIXz+nnLue3FjoiMNGu7O+u2NoegfABUCEdmDssIcHvjXIzhhSinfeuhNbnrqbQ1PMQDqmtqJtHSoRSAiySEvK4N5F8/h3Nlj+J8n1/C9R1bQpTuK9smG6p2jjoajRRD0xDQiMghkpqfx8/NnMjQvizufW0+kpYOfnn9I4PNvD1a7Rh0NwTMEoEIgIn2UlmZ882PTKMzJ5H//tob0NLjh3ENIUzHotw3VjZjBuGGJv3UUVAhEpB/MjC+fNJlOd2566m3S04wfnnOwikE/baxuYnRRLtkZ4ZhjWoVARPrt30+aTGdXF79++h0y0tL4/tkHaQa0fthQ3ciEEeHoKAYVAhH5EMyMr50yhfZOZ/7idew3LI8rj5uU6FhJYeeto2ccPCrRUXZRIRCRD8XMuP60qVTUNvGjx1cyfngepxw0MtGxQq8q0kpdUzsHlg5JdJRddPuoiHxoaWnGLz55KIeMKeLLv1vOm5vrEx0p9FZtjc4Md+DIggQneZ8KgYjsk9ysdG6/ZC5D8zL5/D3lbK5rTnSkUFsdKwRTRxYmOMn7VAhEZJ+VFuZw52WH0djWwWfveImqSGuiI4XWqq0RSgqyGZaflegou6gQiMiAmDaqkLsvP4yt9S1cfOdL1DW1JTpSKK2ubGBqiC4LgQqBiAygOeOHcfslc1lX1chlC16hsbUj0ZFCpbPLebtyB1PKVAhEZBA7ZvIIbv7MLF6vqOM/fv+aBqnrZmN1I60dXUxRi0BEBrtTDhrJf50xjUVvbeWWZ95JdJzQ2NlRnFKFwMyKzWyhma0ys5VmdmSP9WZmN5nZWjN73cxmB5lHROLnimMmcs6ho/n5E6t5etW2RMcJhVVbI5jB5NIUKgTAjcAid58KzARW9lh/OjA59roKuDXgPCISJ2bGj889hGkjC/nS75axfntjoiMl3OqtESYMzyc3KxxjDO0UWCEwsyLgOOBOAHdvc/e6HpudDdzrUS8CxWYWnueuRWSf5GalM/+SOWSmp3HpXS9T2dCS6EgJtboyErqOYgi2RTARqAIWmNkyM7vDzHoOvj0G2NTtfUVs2QeY2VVmVm5m5VVVVcElFpEBN3ZoHgsuO4yaxjYuuuMlqnek5jMGzW2dbKhuDF3/AARbCDKA2cCt7j4LaASu/zAHcvf57j7X3eeWlJQMZEYRiYOZ44q589K5bKpp4pK7Xqa+uT3RkeLu7W0R3AndMwQQbCGoACrc/aXY+4VEC0N3m4Fx3d6PjS0TkUHm8EnDue3iOaypjHD5gpeJtKRWMVgV0juGIMBC4O5bgU1mNiW26ERgRY/NHgYuid09dARQ7+5bgsokIol1/JRSfnXhbF6vqOfSu1KrGKzZGiE7I43xIZmnuLug7xq6FrjfzF4HDgV+ZGZXm9nVsfWPAeuAtcDtwBcDziMiCXbajJGxB85SqxisrowwuWxIKOd5DnQ+AndfDsztsXhet/UOXBNkBhEJn9NmjOLmz8C//WYZl971MvddcTj52YN7epRVWyMcNzmcfZx6slhEEiJaDGbxWkU9V91XTkt7Z6IjBaamsY2qSGsoO4pBhUBEEui0GaP46XmHsGRtNV/67TI6OrsSHSkQL62rBqJ3T4WRCoGIJNR5c8bynY9P54kVlVz/4Bt0dQ2+QeqeWV1FQU4Gs/cLZyEY3BflRCQpXH70ROqb2/nl394mNzOd7599EGbh61T9MNydZ9Zs47jJJWSkh/N3bxUCEQmFL584mea2Tm5bvI70NOM7H58+KIrByi0RKhta+eiUcHYUgwqBiISEmXH96VPp6HLufG496WnGNz82LemLwdOroyOvHn+gCoGIyF6ZRb/8O7sVg2+cPjWpi8E/Vldx0OhCSgtzEh1lt1QIRCRUzKKXhbrcmb94He2dXXz7zOS8TFTf3M7Sd2v5wkf3T3SUPVIhEJHQMTO+d9ZBpKcZC5ZsoLPL+d5ZydeB/Nzb2+nsco4Pcf8AqBCISEiZGd8+czqZ6WmxloHzw3NmkBbCIRp255nV2yjKzeTQkD4/sJMKgYiEllm0jyAz3fj10+/Q0t7Jz84/JLS3YXbX1eU8s6aKYyePCH1eFQIRCTUz4+unTiUvK4Of/XU1zW2d3HjhoWRnhGu6x55WbGmgKtLKCVNKEx1lr8JdpkREYq454QC+8/HpLHprK1fdu5TmtnCPTfTQss2kp1monx/YSYVARJLG5UdP5CfnHczit6u4NMST2+xo7eCBVzZxxsGjGDEkO9Fx9kqFQESSygWH7cdNn57Fqxtr+ewdL1HX1JboSP/k969sItLawRXHTEx0lD5RIRCRpPPxmaOZ99k5rNwa4YLbXmRLfXOiI+3S2eUseH49c8cPDf3dQjupEIhIUjppehkLLjuMitomzrp5CcverU10JACeXFHJpprmpGkNgAqBiCSxow8YwZ+uOZqczDQumP8iDy3bnOhI3PXcesYOzeWUg0YmOkqfqRCISFI7sKyAP19zDLPGFfOVB5bz/UdW0NqRmDuK3qio5+UNNVx21IRQzk28OyoEIpL0huVncd8Vh3PZURO4a8l6zrv1edZV7Yhrho7OLm5YtJIh2RlccNi4uJ57X6kQiMigkJWRxnfPOojbL5lLRW0zZ/7qOX5fvgn3+Mx4dsPjq1iytppvnTmNgpzMuJxzoKgQiMigcvL0Mh7/8rEcPKaI/1z4Olfeu5SqSGug5/zj0grueG49lx45ngsO2y/QcwUh0EJgZhvM7A0zW25m5b2sLzKzR8zsNTN7y8wuDzKPiKSGUUW5/PbKI/jmx6ax+O0qTv3lYh59fUsgrYPlm+r4xp/e4MhJw/nmmdMH/PjxEI8WwQnufqi7z+1l3TXACnefCRwP/MLMsuKQSUQGubQ04/PHTuLRa49hTHEu1/zmVS5d8MqA9h0se7eWz939CqUF2fz6otlkhnxwud1JdGoHCiw6yPgQoAboSGwkERlMJpcV8OAXj+LbZ05n2cZaTv3lYm54fBW1jfv2RPITb23lwttfZEh2BvddcTjD8pP3d1gLsiPFzA8as74AAAjQSURBVNYDtUS/8G9z9/k91hcADwNTgQLgAnd/tJfjXAVcBbDffvvN2bhxY2CZRWTwqoq08pNFq1i4tIKczDTOnzOWK46ZxMQR+X0+RmeXc8/zG/jBoys4ZGwxd146NynGEzKzpbu5MhN4IRjj7pvNrBR4ErjW3Rd3W38+cDTwVWD/2DYz3b1hd8ecO3eul5f/U3eDiEiframMcMez63ho2Xu0dXYxsjCH0cU5jCrOZcboIk6cVsrk0iEfmBGtvrmdP5Rv4t4XNvJuTRMnTSvjpgsPJS8rOUbzT1gh6BHiu8AOd/95t2WPAje4+7Ox938Hrnf3l3d3HBUCERko2yItLFxawbqqRrbUN7O5tpkN1U0AjBuWy/RRhdQ2tVPT2MammiZaO7o4bMJQLj1qAqfPGJVUD43tqRAEVsrMLB9Ic/dI7OdTgO/32Oxd4ETgWTMrA6YA64LKJCLSXWlBDl88/oAPLNta38JTqyp5auU23qlqZFh+FpNLh3Dc5BLOnT2GGWOKEpQ2OEG2acqAP8WaVhnAb9x9kZldDeDu84AfAHeb2RuAAde5+/YAM4mI7NHIohwuOnw8Fx0+PtFR4iawQuDu64CZvSyf1+3n94i2FEREJEESffuoiIgkmAqBiEiKUyEQEUlxKgQiIilOhUBEJMWpEIiIpDgVAhGRFBe3ISYGiplVATtHnSsC6vfwc89lmUB/H1jrfoy+rOu5rK8Zd/53RD8zxivfzmX6DMOVLxkyhj3fvmTc07KwfYbj3b2k16O7e9K+gPl7+rnnMqB8X87Rl3U9l/U1Y7f/9itjvPLpMwxnvmTIGPZ8+5JxL1lD9Rnu6ZXsl4Ye2cvPu1v/Yc/Rl3U9l/U1Y9jz7e1ce6LPcO/n2ZO97Rf2jGHPt7v1fcm4t2X9EfRnuFtJd2loX5hZue9m9L2wCHvGsOeD8GcMez4If8aw54PkyLhTsrcI+mv+3jdJuLBnDHs+CH/GsOeD8GcMez5IjoxAirUIRETkn6Vai0BERHpQIRARSXEqBCIiKU6FIMbMjjWzeWZ2h5k9n+g8vTGzNDP7oZn9yswuTXSenszseDN7NvY5Hp/oPL0xs3wzKzezMxOdpTdmNi32+S00sy8kOk9vzOwcM7vdzB4ws9BNLGVmk8zsTjNbmOgsO8X+3d0T+9wuSnSengZFITCzu8xsm5m92WP5aWa22szWmtn1ezqGuz/r7lcDfwHuCWNG4GxgLNAOVIQwnwM7gJyQ5gO4Dvj9QGYbyIzuvjL27/BTwNEhzfiQu18JXA1cEMJ869z9ioHM1Zt+Zj0XWBj73M4KOlu/9efJt7C+gOOA2cCb3ZalA+8Ak4As4DVgOnAw0S/77q/Sbvv9HigIY0bgeuBfY/suDGG+tNh+ZcD9Icx3MvBp4DLgzDD+Hcf2OQt4HPhMWDPG9vsFMDvE+Qb0/5F9zPoN4NDYNr8JMteHeQU5eX3cuPtiM5vQY/FHgLUenTsZM/sdcLa7/xjo9bKAme0H1Lt7JIwZzawCaIu97Qxbvm5qgeyw5Ytdrson+j9ms5k95u5dYcoYO87DwMNm9ijwm4HKN1AZzcyAG4DH3f3VsOWLl/5kJdpCHgssJ4RXYgZFIdiNMcCmbu8rgMP3ss8VwILAEv2z/mZ8EPiVmR0LLA4yWEy/8pnZucCpQDFwc7DRgH7mc/f/B2BmlwHbB7II7EF/P8PjiV5GyAYeCzTZ+/r77/Ba4CSgyMwOcPd5QYaj/5/hcOCHwCwz+0asYMTL7rLeBNxsZh/jww9BEZjBXAj6zd2/k+gMe+LuTUSLVSi5+4NEi1Woufvdic6wO+7+DPBMgmPskbvfRPSLLZTcvZpo/0VouHsjcHmic+xO6JooA2gzMK7b+7GxZWES9ozKt++Ucd+FPV93yZR1l8FcCF4BJpvZRDPLItpJ+HCCM/UU9ozKt++Ucd+FPV93yZT1fYnurR6g3vvfAlt4/7bKK2LLzwDWEO3F/3/KqHzKGO6MYc+XrFn39tKgcyIiKW4wXxoSEZE+UCEQEUlxKgQiIilOhUBEJMWpEIiIpDgVAhGRFKdCIIOCme2I8/kGZM4Ki87hUG9my81slZn9vA/7nGNm0wfi/CKgQiDSKzPb4zhc7n7UAJ7uWXc/FJgFnGlme5uH4ByiI6iKDAgVAhm0zGx/M1tkZkstOnPa1Njyj5vZS2a2zMz+ZmZlseXfNbP7zGwJcF/s/V1m9oyZrTOzL3U79o7Yf4+PrV8Y+43+/tgwzZjZGbFlS83sJjP7y57yunsz0WGKx8T2v9LMXjGz18zsj2aWZ2ZHEZ2v4GexVsT+u/tzivSVCoEMZvOBa919DvA14JbY8ueAI9x9FvA74D+77TMdOMndL4y9n0p0aO2PAN8xs8xezjML+Eps30nA0WaWA9wGnB47f8newprZUGAy7w8x/qC7H+buM4GVRIcweJ7o2DVfd/dD3f2dPfw5RfpEw1DLoGRmQ4CjgD/EfkGH9yfLGQs8YGajiM4itb7brg/HfjPf6VF3bwVazWwb0dnXek7D+bK7V8TOuxyYQHTKznXuvvPYvwWu2k3cY83sNaJF4JfuvjW2fIaZ/TfR+R2GAH/t559TpE9UCGSwSgPqYtfee/oV8D/u/nBsIpjvdlvX2GPb1m4/d9L7/zN92WZPnnX3M81sIvCimf3e3ZcDdwPnuPtrscl0ju9l3z39OUX6RJeGZFBy9wZgvZl9EqLTK5rZzNjqIt4fI/7SgCKsBiZ1m8pwr5O8x1oPNwDXxRYVAFtil6Mu6rZpJLZub39OkT5RIZDBIs/MKrq9vkr0y/OK2GWXt4jOHQvRFsAfzGwpsD2IMLHLS18EFsXOEwHq+7DrPOC4WAH5FvASsARY1W2b3wFfj3V278/u/5wifaJhqEUCYmZD3H1H7C6iXwNvu/v/JjqXSE9qEYgE58pY5/FbRC9H3ZbgPCK9UotARCTFqUUgIpLiVAhERFKcCoGISIpTIRARSXEqBCIiKU6FQEQkxf1/F4KUawBr0sIAAAAASUVORK5CYII=%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When we created our <code>Learner</code> the embeddings from the pre-trained AWD_LSTM model were merged with random embeddings added for words that weren't in the vocabulary. The pre-trained layers were also automatically frozen for us. Using <code>fit_one_cycle</code> with our <code>Learner</code> will train only the <em>new random embeddings</em> (i.e. words that are in our Twitter vocab but not the Wikipedia vocab) in the last layer of the neural network.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>perplexity</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>4.390779</td>
      <td>4.256468</td>
      <td>0.257898</td>
      <td>70.560310</td>
      <td>03:45</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After one epoch our language model is predicting the next word in a tweet around 25% of the time - not too bad! We can <code>unfreeze</code> the entire model, find a more suitable learning rate and train for a few more epochs to improve the accuracy further.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.0002511886414140463, lr_steep=7.585775847473997e-07)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>perplexity</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>3.960577</td>
      <td>4.097842</td>
      <td>0.279537</td>
      <td>60.210228</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>1</td>
      <td>3.799337</td>
      <td>4.013978</td>
      <td>0.290340</td>
      <td>55.366707</td>
      <td>04:20</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3.624613</td>
      <td>4.001914</td>
      <td>0.295858</td>
      <td>54.702751</td>
      <td>04:19</td>
    </tr>
    <tr>
      <td>3</td>
      <td>3.470943</td>
      <td>4.027544</td>
      <td>0.295395</td>
      <td>56.122932</td>
      <td>04:20</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After a bit more training we can predict the next word in a tweet around 29% of the time. Let's test the model out by using it to write some random tweets (in this case it will generate some text following 'I love').</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">TEXT</span> <span class="o">=</span> <span class="s2">"I love"</span>
<span class="n">N_WORDS</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">N_SENTENCES</span> <span class="o">=</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">N_WORDS</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_SENTENCES</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>i love it when your back is full ! i love it , and i liked it ! I 'm not talking about that to anyone else .
i love this one Although i can see the difference in the way , I 'm sure i ca n't get it . first shot DONE !
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Some interesting results there! Let's save the model <em>encoder</em> so we can use it to fine-tune our classifier. The encoder is all of the model except for the final layer, which converts activations to probabilities of picking each token in the vocabulary. We want to keep the knowledge the model has learned about tweet language but we won't be using our classifier to predict the next word in a sentence, so we won't need the final layer any more.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save_encoder</span><span class="p">(</span><span class="s1">'finetuned_lm'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-a-sentiment-classifier">
<a class="anchor" href="#Training-a-sentiment-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training a sentiment classifier<a class="anchor-link" href="#Training-a-sentiment-classifier"> </a>
</h1>
<p>To get the <code>DataLoaders</code> for our classifier let's use the <code>DataBlock</code> API this time, which is more customisable.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls_clas</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextBlock</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="s1">'text'</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">dls_lm</span><span class="o">.</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">dls_lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span>
              <span class="n">CategoryBlock</span><span class="p">),</span>
    <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">'text'</span><span class="p">),</span>
    <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">'sentiment'</span><span class="p">),</span>
    <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">()</span>
<span class="p">)</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df_clas</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To use the API, <code>fastai</code> needs the following:</p>
<ul>
<li>
<code>blocks</code>:<ul>
<li>
<code>TextBlock</code>: Our x variable will be text contained in a <code>pandas</code> <code>DataFrame</code>. We want to use the same sequence length and vocab as the language model <code>DataLoaders</code> so we can make use of our pre-trained model.</li>
<li>
<code>CategoryBlock</code>: Our y variable will be a single-label category (negative, neutral or positive sentiment).</li>
</ul>
</li>
<li>
<code>get_x</code>, <code>get_y</code>: Get data for the model by reading the <code>text</code> and <code>sentiment</code> columns from the <code>DataFrame</code>.</li>
<li>
<code>splitter</code>: We will use <code>RandomSplitter()</code> to randomly split the data into a training set (80% by default) and a validation set (20%).</li>
<li>
<code>dataloaders</code>: Builds the <code>DataLoaders</code> using the <code>DataBlock</code> template we just defined, the <em>df_clas</em> <code>DataFrame</code> and a batch size of 64.</li>
</ul>
<p>We can call show batch as before; this time the dependent variable is sentiment.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls_clas</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>xxbos xxup pirate xxup voice : xxrep 3 a xxrep 3 r xxrep 3 g xxrep 3 h xxrep 3 ! i 4got xxup my xxup damn xxup wallet xxup at xxup work xxup xxunk xxrep 3 ! xxup dammit xxrep 3 ! xxup so xxup close xxup yet xxup so xxup far xxrep 3 ! xxup now xxup i m xxup starving xxrep 3 !</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>xxbos xxup ugg xxup want xxup to xxup go xxup to xxup xxunk xxup house xxup but i xxup ca nt xxup finna xxup be xxup bored xxup this xxup weekend xxrep 3 ! xxrep 3 u xxup xxunk xxup wanna xxup spend xxup da xxup nite xxup and xxup go xxup see xxup up xxup and xxup go xxup shopping</td>
      <td>neutral</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Initialising the <code>Learner</code> is similar to before, but in this case we want a <code>text_classifier_learner</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">text_classifier_learner</span><span class="p">(</span><span class="n">dls_clas</span><span class="p">,</span> <span class="n">AWD_LSTM</span><span class="p">,</span> <span class="n">drop_mult</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we want to load the encoder from the language model we trained earlier, so our classifier uses pre-trained weights.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="s1">'finetuned_lm'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-tuning-the-classifier">
<a class="anchor" href="#Fine-tuning-the-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fine-tuning the classifier<a class="anchor-link" href="#Fine-tuning-the-classifier"> </a>
</h2>
<p>Now we can train the classifier using <em>discriminative learning rates</em> and <em>gradual unfreezing</em>, which has been found to give better results for this type of model. First let's freeze all but the last layer:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.828995</td>
      <td>0.759167</td>
      <td>0.666028</td>
      <td>00:54</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now freeze all but the last two layers:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">1e-2</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">1e-2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.733606</td>
      <td>0.615954</td>
      <td>0.739144</td>
      <td>00:56</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now all but the last three:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">5e-3</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">5e-3</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.650302</td>
      <td>0.566739</td>
      <td>0.763570</td>
      <td>01:07</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, let's unfreeze the entire model and train a bit more:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">1e-3</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">1e-3</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.606317</td>
      <td>0.567328</td>
      <td>0.767401</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.558857</td>
      <td>0.560174</td>
      <td>0.766762</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.527293</td>
      <td>0.562808</td>
      <td>0.766922</td>
      <td>01:24</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">'classifier'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path('models/classifier.pth')</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our model correctly predicts sentiment around 77% of the time. We could perhaps do better with a larger dataset as mentioned earlier, or different model hyperparameters. It might be worth experimenting with this yourself to see if you can improve the accuracy.</p>
<p>We can quickly sense check the model by calling <code>predict</code>, which returns the predicted sentiment, the index of the prediction and predicted probabilities for negative, neutral and positive sentiment.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"I love"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>('positive', tensor(2), tensor([0.0025, 0.0041, 0.9934]))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">"I hate"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>('negative', tensor(0), tensor([0.9889, 0.0071, 0.0040]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Classifying-unlabelled-tweets">
<a class="anchor" href="#Classifying-unlabelled-tweets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classifying unlabelled tweets<a class="anchor-link" href="#Classifying-unlabelled-tweets"> </a>
</h1>
<p>To carry out sentiment analysis on the vaccine tweets, we can add them to the <code>DataLoaders</code> as a test set:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pred_dl</span> <span class="o">=</span> <span class="n">dls_clas</span><span class="o">.</span><span class="n">test_dl</span><span class="p">(</span><span class="n">vax_tweets</span><span class="p">[</span><span class="s1">'text'</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can then make predictions using <code>get_preds</code>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">get_preds</span><span class="p">(</span><span class="n">dl</span><span class="o">=</span><span class="n">pred_dl</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can save the results for analysis later.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vax_tweets</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">vax_tweets</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span> <span class="o">=</span> <span class="n">vax_tweets</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span><span class="s1">'negative'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="s1">'neutral'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="s1">'positive'</span><span class="p">})</span>

<span class="c1"># Convert dates</span>
<span class="n">vax_tweets</span><span class="p">[</span><span class="s1">'date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">vax_tweets</span><span class="p">[</span><span class="s1">'date'</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">'coerce'</span><span class="p">)</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">date</span>

<span class="c1"># Save to csv</span>
<span class="n">vax_tweets</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">'vax_tweets_inc_sentiment.csv'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h1>
<p><code>fastai</code> make NLP really easy, and we were able to get quite good results with a limited dataset and not a lot of training time by using the ULMFiT approach. To summarise, the steps are:</p>
<ol>
<li>Fine-tune a language model to predict the next word in a tweet, using a model pre-trained on Wikipedia.</li>
<li>Fine-tune a classification model to predict tweet sentiment using the pre-trained language model.</li>
<li>Apply the classifier to unlabelled tweets to analyse sentiment.</li>
</ol>
<p>In <a href="https://thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/visualisation/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-2.html">part 2</a> we will use our new model for analysis, investigating the overall sentiment of each vaccine, how sentiment changes over time and the relationship between sentiment and vaccination progress in different countries.</p>
<p>I hope you found this useful, and thanks very much to <a href="https://www.kaggle.com/gpreda">Gabriel Preda</a> for providing the data!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="footnotes"><p id="fn-1">1. Cover image via <a href="https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/">https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/</a><a href="#fnref-1" class="footnote footnotes">‚Ü©</a></p></div>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="twhelan22/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastai/nlp/sentiment%20analysis/pytorch/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog about data science and machine learning.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/twhelan22" title="twhelan22"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/tom_whelan" title="tom_whelan"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
