{
  
    
        "post0": {
            "title": "COVID-19 Vaccine Tweet Sentiment Analysis with fastai - Part 2",
            "content": "Analysing overall sentiment . In part 1, we trained a sentiment classification model and used it to predict the sentiment of tweets about COVID-19 vaccines. Our focus in this part will be to analyse the results from our model. . Note: As mentioned in part 1, this is a write-up of a submission I made for several Kaggle tasks, which are still open and accepting new entries at the time of writing if you want to give them a go yourself! See the conclusion for some ideas. . First, let&#39;s load in the data from part 1 and plot the frequency of each sentiment. . vax_tweets = pd.read_csv(&#39;https://raw.githubusercontent.com/twhelan22/blog/master/data/vax_tweets_inc_sentiment.csv&#39;, index_col=0, parse_dates=[&#39;date&#39;]) # Plot sentiment value counts vax_tweets[&#39;sentiment&#39;].value_counts(normalize=True).plot.bar(title=&#39;COVID-19 vaccine tweet sentiment&#39;); . . We can see that the predominant sentiment is neutral, with more positive tweets than negative. It&#39;s encouraging that negative sentiment isn&#39;t higher! We can also visualise how sentiment changes over time: . # Get counts of number of tweets by sentiment for each date timeline = vax_tweets.groupby([&#39;date&#39;, &#39;sentiment&#39;]).agg(**{&#39;tweets&#39;: (&#39;id&#39;, &#39;count&#39;)}).reset_index().dropna() # Plot results fig = px.line(timeline, x=&#39;date&#39;, y=&#39;tweets&#39;, color=&#39;sentiment&#39;, category_orders={&#39;sentiment&#39;: [&#39;neutral&#39;, &#39;negative&#39;, &#39;positive&#39;]}, title=&#39;Timeline showing sentiment of tweets about COVID-19 vaccines&#39;) fig.show() . . . . There was a big spike in the number of tweets on March 1st 2021, so let&#39;s investigate further. A lot of the tweets appear to be from users in India: . spike = vax_tweets[vax_tweets[&#39;date&#39;].astype(str)==&#39;2021-03-01&#39;] spike[&#39;user_location&#39;].value_counts(ascending=False).head(10) . . India 258 New Delhi, India 138 patna 52 Mumbai, India 48 New Delhi 46 Bengaluru, India 32 Mumbai 28 Delhi 26 Hyderabad, India 24 Pune, India 22 Name: user_location, dtype: int64 . spike = spike.sort_values(&#39;user_location&#39;, ascending=False) spike[&#39;orig_text&#39;].head() . . 18084 Before magreact, do the research how the vacci... 17555 I find this Photo by @cpimspeak nTo be offensi... 15285 üáÆüá≥ PM Shri @narendramodi took his first dose o... 16532 Got call at 9 am from health department and mo... 16901 #mRNAvaccine #PfizerBionTech n#Moderna #Katali... Name: orig_text, dtype: object . It looks like Indian Prime Minister Narendra Modi received the first dose of Indian developed Covaxin on 1st March. No wonder there were lots of tweets! To dig deeper, let&#39;s plot timelines for each vaccine indvidually. . Timeline analysis for each vaccine . Covaxin . all_vax = [&#39;covaxin&#39;, &#39;sinopharm&#39;, &#39;sinovac&#39;, &#39;moderna&#39;, &#39;pfizer&#39;, &#39;biontech&#39;, &#39;oxford&#39;, &#39;astrazeneca&#39;, &#39;sputnik&#39;] # Function to filter the data to a single vaccine # Note: a lot of the tweets seem to contain hashtags for multiple vaccines even though they are specifically referring to one vaccine - not very helpful! def filtered_df(df, vax): df = df.dropna() df_filt = pd.DataFrame() for o in vax: df_filt = df_filt.append(df[df[&#39;orig_text&#39;].str.lower().str.contains(o)]) other_vax = list(set(all_vax)-set(vax)) for o in other_vax: df_filt = df_filt[~df_filt[&#39;orig_text&#39;].str.lower().str.contains(o)] df_filt = df_filt.drop_duplicates() return df_filt # Function to plot the timeline def plot_timeline(df, title): title_str = &#39;Timeline showing sentiment of tweets about the &#39;+title+&#39; vaccine&#39; timeline = df.groupby([&#39;date&#39;, &#39;sentiment&#39;]).agg(**{&#39;tweets&#39;: (&#39;id&#39;, &#39;count&#39;)}).reset_index() fig = px.line(timeline, x=&#39;date&#39;, y=&#39;tweets&#39;, color=&#39;sentiment&#39;, category_orders={&#39;sentiment&#39;: [&#39;neutral&#39;, &#39;negative&#39;, &#39;positive&#39;]}, title=title_str) fig.show() covaxin = filtered_df(vax_tweets, [&#39;covaxin&#39;]) plot_timeline(covaxin, title=&#39;Covaxin&#39;) . . . . # Function to filter the data to a single date and print tweets from users with the most followers def date_filter(df, date): return df[df[&#39;date&#39;].astype(str)==date].sort_values(&#39;user_followers&#39;, ascending=False)[[&#39;date&#39; ,&#39;orig_text&#39;]] def date_printer(df, dates, num=10): for date in dates: display(date_filter(df, date).head(num)) date_printer(covaxin, [&#39;2021-03-01&#39;, &#39;2021-03-03&#39;]) . . date orig_text . 18936 2021-03-01 | &quot;Felt secure, will travel safely&quot; EAM @DrSJais... | . 17463 2021-03-01 | #Watch | PM @NarendraModi was administered the... | . 13382 2021-03-01 | @nistula Sources in the govt say PM #NarendraM... | . 13107 2021-03-01 | PM #NarendraModi took the first shot of #COVAX... | . 18912 2021-03-01 | There are two #CovidVaccines that are being us... | . 18960 2021-03-01 | External Affairs Minister Jaishankar receives ... | . 18750 2021-03-01 | A 100-year-old resident of #Hyderabad, Jaidev ... | . 18700 2021-03-01 | #PMModi took the first does of #Covid19 vaccin... | . 18666 2021-03-01 | #PMModi took the first dose of #Covaxin today.... | . 18803 2021-03-01 | #PMModi flagged off the second phase of #Covid... | . date orig_text . 20792 2021-03-03 | #Covaxin 81% Effective, Works Against UK Varia... | . 20403 2021-03-03 | ‚ÄúThe numbers are extremely promising at this s... | . 20388 2021-03-03 | ‚ÄúThe data is quite encouraging‚Äù: Dr Rachna Kuc... | . 20696 2021-03-03 | #Covaxin&#39;s Phase 3 Trial Results Out! #Covid19... | . 20411 2021-03-03 | For those like me who were concerned that #Cov... | . 20563 2021-03-03 | #Covaxin demonstrates the prowess of Atmanirbh... | . 20349 2021-03-03 | India&#39;s vaccine maker Bharat Biotech said Wed ... | . 20850 2021-03-03 | Bharat Biotech announces phase 3 results of Co... | . 20380 2021-03-03 | #Covaxin is one of the two vaccines that have ... | . 20671 2021-03-03 | .@BharatBiotech announces the phase 3 results ... | . Modi wasn&#39;t the only person to make news on March 1st; India&#39;s External Affairs Minister and a 100-year-old Hyderabad resident also received their first dose of Covaxin. On March 3rd, phase 3 trial results for Covaxin were published, showing 81% efficacy. It makes sense for there to be a spike in the number of neutral and positive tweets about Covaxin on those dates! . Sinovac . sinovac = filtered_df(vax_tweets, [&#39;sinovac&#39;]) plot_timeline(sinovac, title=&#39;Sinovac&#39;) . . . . Some notable dates: . date_printer(sinovac, [&#39;2021-02-22&#39;, &#39;2021-02-28&#39;, &#39;2021-03-01&#39;, &#39;2021-03-03&#39;, &#39;2021-03-08&#39;], 3) . . date orig_text . 11715 2021-02-22 | Thai PM Prayut Chan-o-cha possibly among first... | . 11757 2021-02-22 | Carrie Lam, Chief Executive of #HongKong SAR, ... | . 11765 2021-02-22 | The #Philippines has officially approved the e... | . date orig_text . 16270 2021-02-28 | #Thai deputy PM and ministers are part of the ... | . 16253 2021-02-28 | China has provided Mexico with 1 million doses... | . 16254 2021-02-28 | Second batch of #Sinovac vaccines produced by ... | . date orig_text . 16806 2021-03-01 | #Philippine General Hospital (PGH) Director Dr... | . 16779 2021-03-01 | The #Philippines kicked off vaccination drive ... | . 16818 2021-03-01 | A batch of #Sinovac #vaccine donated by China ... | . date orig_text . 19152 2021-03-03 | Brazilian soccer legend #Pele on Tuesday recei... | . 19162 2021-03-03 | In pics: Raw materials for China&#39;s #Sinovac #C... | . 19175 2021-03-03 | It is extremely unlikely that the death of a 6... | . date orig_text . 23448 2021-03-08 | The second batch of China&#39;s #Sinovac COVID-19 ... | . 23834 2021-03-08 | China&#39;s #Sinovac #covid19 #vaccines show an 80... | . 23836 2021-03-08 | #Sinovac‚Äôs #vaccine shows an 80-90% efficacy r... | . These tweets are about countries starting their vaccination programme or receiving a new shipment of vaccines. Let&#39;s use the &#39;COVID-19 World Vaccination Progress&#39; dataset to plot daily vaccinations for the mentioned countries: . vax_progress = pd.read_csv(&#39;https://raw.githubusercontent.com/twhelan22/blog/master/data/country_vaccinations.csv&#39;, index_col=0, parse_dates=[&#39;date&#39;]) countries = [&#39;Brazil&#39;, &#39;Thailand&#39;, &#39;Hong Kong&#39;, &#39;Colombia&#39;, &#39;Mexico&#39;, &#39;Philippines&#39;, &#39;Indonesia&#39;] fig = px.line(vax_progress[vax_progress[&#39;country&#39;].isin(countries)], x=&#39;date&#39;, y=&#39;daily_vaccinations_per_million&#39;, color=&#39;country&#39;, title=&#39;Daily vaccinations per million (all vaccines) in selected countries&#39;) fig.show() . . . . We can see that daily vaccinations per million increased significantly in Colombia and Mexico after they received new shipments of vaccines. Daily vaccinations are also increasing rapidly in Hong Kong after Carrie Lam received the vaccine on February 22nd; however, progress has been slower in Thailand and the Philippines so far. . Sinopharm . sinopharm = filtered_df(vax_tweets, [&#39;sinopharm&#39;]) plot_timeline(sinopharm, title=&#39;Sinopharm&#39;) . . . . As with Sinovac, most of the Sinopharm tweets appear to be positive news regarding countries receiving a shipment of the vaccine: . date_printer(sinopharm, [&#39;2021-02-18&#39;, &#39;2021-02-24&#39;, &#39;2021-03-02&#39;], 3) . . date orig_text . 9905 2021-02-18 | #Senegal received its #COVID19 vaccines purcha... | . 10391 2021-02-18 | Nepal has granted approval to China‚Äôs #Sinopha... | . 10380 2021-02-18 | With the #Sinopharm #vaccine, Hungarians will ... | . date orig_text . 12655 2021-02-24 | #Sinopharm&#39;s second COVID-19 vaccine has a 72.... | . 13958 2021-02-24 | The first batch of China&#39;s #Sinopharm vaccine ... | . 12680 2021-02-24 | #Senegal on Tuesday officially began the first... | . date orig_text . 17972 2021-03-02 | The first batch of #Sinopharm #COVID19 vaccine... | . 17977 2021-03-02 | China will provide 50,000 inactivated #Sinopha... | . 17945 2021-03-02 | #Iraq received its first 50,000 doses of the #... | . countries = [&#39;Senegal&#39;, &#39;Nepal&#39;, &#39;Hungary&#39;, &#39;Bolivia&#39;, &#39;Lebanon&#39;] fig = px.line(vax_progress[vax_progress[&#39;country&#39;].isin(countries)], x=&#39;date&#39;, y=&#39;daily_vaccinations_per_million&#39;, color=&#39;country&#39;, title=&#39;Daily vaccinations per million (all vaccines) in selected countries&#39;) fig.show() . . . . We can see that Hungary ramped up their vaccination programme after the news on February 18th that they would become the first EU country to start administering Sinopharm. In addition, Senegal started vaccinating shortly after positive tweets confirmed that they had received a shipment of Sinopharm vaccines. Unfortunately there is no data for Iraq, but they also started their programme just hours after receiving a donation of vaccines from China. . Moderna . moderna = filtered_df(vax_tweets, [&#39;moderna&#39;]) plot_timeline(moderna, title=&#39;Moderna&#39;) . . . . Some notable dates: . date_printer(moderna, [&#39;2021-02-17&#39;, &#39;2021-03-05&#39;, &#39;2021-03-11&#39;], 3) . . date orig_text . 9458 2021-02-17 | #UPDATE The European Union has bought up to 30... | . 9464 2021-02-17 | #Covid19: EU approves contract for 300 million... | . 9471 2021-02-17 | üíâüá™üá∫ The European Commission said on Wednesday ... | . date orig_text . 22413 2021-03-05 | #Japan‚Äôs Takeda Pharmaceutical Co asks regulat... | . 22422 2021-03-05 | #Moderna To Collaborate With IBM On #COVID19Va... | . 21191 2021-03-05 | Moderna COVID-19 Vaccine Recipients Experience... | . date orig_text . 27203 2021-03-11 | @mpetrillo59 It was the #Moderna. | . 27207 2021-03-11 | I got the #CovidVaccine today. nI received the... | . 27370 2021-03-11 | üá∫üá∏Utah mother, 39, with NO known health issues... | . On March 2nd Dolly Parton received her dose of the vaccine she helped fund, which explains the initial increase in positive tweets prior to the news about Moderna&#39;s collaboration with IBM. By looking at the vaccine progress, we can see that the median daily vaccinations per million in EU countries started to pull further ahead of the rest of the world after news that they would purchase up to 300m extra Moderna vaccines: . countries = [&#39;Austria&#39;, &#39;Belgium&#39;, &#39;Bulgaria&#39;, &#39;Croatia&#39;, &#39;Cyprus&#39;, &#39;Czechia&#39;, &#39;Denmark&#39;, &#39;Estonia&#39;, &#39;Finland&#39;, &#39;France&#39;, &#39;Germany&#39;, &#39;Greece&#39;, &#39;Hungary&#39;, &#39;Ireland&#39;, &#39;Italy&#39;, &#39;Latvia&#39;, &#39;Lithuania&#39;, &#39;Luxembourg&#39;, &#39;Malta&#39;, &#39;Netherlands&#39;, &#39;Poland&#39;, &#39;Portugal&#39;, &#39;Romania&#39;, &#39;Slovakia&#39;, &#39;Slovenia&#39;, &#39;Spain&#39;,&#39;Sweden&#39;] eu = vax_progress[vax_progress[&#39;country&#39;].isin(countries)].groupby(&#39;date&#39;)[&#39;daily_vaccinations_per_million&#39;].median().reset_index() eu[&#39;region&#39;] = &#39;EU&#39; row = vax_progress[~vax_progress[&#39;country&#39;].isin(countries)].groupby(&#39;date&#39;)[&#39;daily_vaccinations_per_million&#39;].median().reset_index() row[&#39;region&#39;] = &#39;Rest of world&#39; fig = px.line(eu.append(row), x=&#39;date&#39;, y=&#39;daily_vaccinations_per_million&#39;, color=&#39;region&#39;, title=&#39;Median daily vaccinations per million (all vaccines) in EU countries vs the rest of the world&#39;) fig.add_vline(x=&#39;2021-02-17&#39;, line_width=3, line_dash=&#39;dash&#39;, line_color=&#39;#00cc96&#39;) fig.add_annotation(x=&#39;2021-02-17&#39;, y=2120, text=&quot;EU makes a deal to purchase up to 300m extra Moderna vaccines&quot;, showarrow=True, arrowhead=5, ax=-220, ay=-30) fig.show() . . . . Sputnik V . sputnikv = filtered_df(vax_tweets, [&#39;sputnik&#39;]) plot_timeline(sputnikv, title=&#39;Sputnik V&#39;) . . . . Some notable dates: . date_printer(sputnikv, [&#39;2021-03-04&#39;, &#39;2021-03-05&#39;, &#39;2021-03-10&#39;, &#39;2021-03-11&#39;, &#39;2021-03-15&#39;], 3) . . date orig_text . 21932 2021-03-04 | European Union drug regulator on Thursday star... | . 22012 2021-03-04 | Sputnik V could be India&#39;s third #Covid19 vacc... | . 21991 2021-03-04 | Sputnik V Could Be India‚Äôs 3rd COVID Vaccine: ... | . date orig_text . 22850 2021-03-05 | [Coronavirus] EU&#39;s medicines agency @EMA_News ... | . 22865 2021-03-05 | #SputnikV is now the world&#39;s second most popul... | . 22876 2021-03-05 | Twitter officially Verified #SputnikV account.... | . date orig_text . 30610 2021-03-10 | Iran and Russia will start to jointly produce ... | . 30745 2021-03-10 | #Russia has signed a deal to produce its #Sput... | . 30748 2021-03-10 | #SputnikV has not yet been approved for use in... | . date orig_text . 30512 2021-03-11 | Best #SputnikV4Victory photos will be publishe... | . 30513 2021-03-11 | #SputnikV, approved by 50 countries, brings vi... | . 30494 2021-03-11 | Anti-#covid19 update: n nüá∞üá™Kenya, üá≤üá¶Morocco, üáØ... | . date orig_text . 30153 2021-03-15 | #NewsAlert | #SputnikV production agreements r... | . 30088 2021-03-15 | The developers of the #SputnikV #coronavirus #... | . 30044 2021-03-15 | @Malinka1102 Salam, here is your unroll: #Russ... | . We can see spikes in positive sentiment after various countries agreed to produce the Sputkik V vaccine, and on March 11th after ABC news reported that it was the safest vaccine. . Pfizer/BioNTech . pfizer = filtered_df(vax_tweets, [&#39;pfizer&#39;, &#39;biontech&#39;]) plot_timeline(pfizer, title=&#39;Pfizer/BioNTech&#39;) . . . . There is a lot to unpack here, so to make things easier let&#39;s annotate some of the key dates: . timeline = pfizer.groupby([&#39;date&#39;, &#39;sentiment&#39;]).agg(**{&#39;tweets&#39;: (&#39;id&#39;, &#39;count&#39;)}).reset_index() fig = px.line(timeline, x=&#39;date&#39;, y=&#39;tweets&#39;, color=&#39;sentiment&#39;, category_orders={&#39;sentiment&#39;: [&#39;neutral&#39;, &#39;negative&#39;, &#39;positive&#39;]}, title=&#39;Timeline showing sentiment of tweets about the PfizerBioNTech vaccine&#39;) fig.add_annotation(x=&#39;2020-12-14&#39;, y=timeline[(timeline[&#39;date&#39;]==&#39;2020-12-14&#39;)&amp;(timeline[&#39;sentiment&#39;]==&#39;positive&#39;)][&#39;tweets&#39;].values[0], text=&quot;USA and UK start vaccinating&quot;, showarrow=True, arrowhead=3, ax=55, ay=-210) fig.add_annotation(x=&#39;2020-12-22&#39;, y=timeline[(timeline[&#39;date&#39;]==&#39;2020-12-22&#39;)&amp;(timeline[&#39;sentiment&#39;]==&#39;positive&#39;)][&#39;tweets&#39;].values[0], text=&quot;Joe Biden receives first dose&quot;, arrowhead=3, ax=10, ay=-100) fig.add_annotation(x=&#39;2021-01-08&#39;, y=timeline[(timeline[&#39;date&#39;]==&#39;2021-01-08&#39;)&amp;(timeline[&#39;sentiment&#39;]==&#39;positive&#39;)][&#39;tweets&#39;].values[0], text=&quot;Vaccine shown to resist new variant&quot;, showarrow=True, align=&#39;left&#39;, arrowhead=3, ax=0, ay=-45) fig.add_annotation(x=&#39;2021-01-16&#39;, y=timeline[(timeline[&#39;date&#39;]==&#39;2021-01-16&#39;)&amp;(timeline[&#39;sentiment&#39;]==&#39;negative&#39;)][&#39;tweets&#39;].values[0], text=&quot;23 elderly Norwegians die after vaccine dose&quot;, showarrow=True, align=&#39;left&#39;, arrowhead=3, ax=15, ay=-180) fig.add_annotation(x=&#39;2021-02-19&#39;, y=timeline[(timeline[&#39;date&#39;]==&#39;2021-02-19&#39;)&amp;(timeline[&#39;sentiment&#39;]==&#39;positive&#39;)][&#39;tweets&#39;].values[0], text=&quot;Israeli study shows 85% efficacy after one dose&quot;, showarrow=True, align=&#39;left&#39;, arrowhead=3, ax=-30, ay=-180) fig.add_annotation(x=&#39;2021-02-25&#39;, y=timeline[(timeline[&#39;date&#39;]==&#39;2021-02-25&#39;)&amp;(timeline[&#39;sentiment&#39;]==&#39;positive&#39;)][&#39;tweets&#39;].values[0], text=&quot;Israeli study shows 94% efficacy after two doses&quot;, showarrow=True, align=&#39;left&#39;, arrowhead=3, ax=-20, ay=-130) fig.show() . . . . Oxford/AstraZeneca . oxford = filtered_df(vax_tweets, [&#39;oxford&#39;, &#39;astrazeneca&#39;]) plot_timeline(oxford, title=&#39;Oxford/AstraZeneca&#39;) . . . . Interestingly, there are small positive spikes on February 19th and March 6th, with people tweeting after receiving the vaccine: . date_printer(oxford, [&#39;2021-02-19&#39;, &#39;2021-03-06&#39;], 5) . . date orig_text . 10616 2021-02-19 | Had my 1st dose of the vaccine. Very impressed... | . 11107 2021-02-19 | @nicolab03 Hurrah! Had mine today too. #Oxford... | . 11108 2021-02-19 | Blimey I feel crap. n nBut it‚Äôs totally worth... | . 10617 2021-02-19 | #vaccine study #nurses #volunteers n#oxfordast... | . 11096 2021-02-19 | Our latest paper on doses of the #oxfordastraz... | . date orig_text . 23213 2021-03-06 | ‚ÄúThe #OxfordAstraZeneca #CovidVaccine develop... | . 23216 2021-03-06 | Drive through #Whitstable for #NewHusband #Oxf... | . 22480 2021-03-06 | @BWildeMTL Hope it gets sorted soon Brian. I h... | . 23211 2021-03-06 | Update... If you look closely, you&#39;ll see wher... | . 22450 2021-03-06 | EU seeks to access AstraZeneca vaccines produc... | . However, negative sentiment is increasing after numerous countries have suspended the use of the vaccine over safety concerns. We can see that vaccination progress in these countries has slowed significantly over the past few days as a result: . # At the time of writing, these countries have completely suspended the use of the vaccine # Note that several other countries continued mostly as normal but suspended the use of one batch of Oxford/AstraZeneca vaccines countries = [&#39;Germany&#39;, &#39;France&#39;, &#39;Spain&#39;, &#39;Italy&#39;, &#39;Netherlands&#39;, &#39;Ireland&#39;, &#39;Denmark&#39;, &#39;Norway&#39;, &#39;Bulgaria&#39;, &#39;Iceland&#39;, &#39;Thailand&#39;] ox_prog = vax_progress[vax_progress[&#39;country&#39;].isin(countries)].groupby(&#39;date&#39;)[&#39;daily_vaccinations_per_million&#39;].median().reset_index() ox_prog[&#39;Use of Oxford/AstraZeneca&#39;] = &#39;Suspended&#39; other_prog = vax_progress[vax_progress[&#39;vaccines&#39;].str.contains(&#39;Oxford/AstraZeneca&#39;)] other_prog = vax_progress[~vax_progress[&#39;country&#39;].isin(countries)].groupby(&#39;date&#39;)[&#39;daily_vaccinations_per_million&#39;].median().reset_index() other_prog[&#39;Use of Oxford/AstraZeneca&#39;] = &#39;Ongoing&#39; fig = px.line(ox_prog.append(other_prog), x=&#39;date&#39;, y=&#39;daily_vaccinations_per_million&#39;, color=&#39;Use of Oxford/AstraZeneca&#39;, title=&quot;Median daily vaccinations per million (all vaccines) in countries that have completely suspended the use of the &lt;br&gt;Oxford/AstraZeneca vaccine vs countries that continue to use it&quot;) fig.add_vrect(x0=&quot;2021-03-11&quot;, x1=&quot;2021-03-15&quot;, annotation_text=&quot;vaccine&lt;br&gt;suspended&quot;, annotation_position=&quot;bottom right&quot;, fillcolor=&quot;limegreen&quot;, opacity=0.25, line_width=0) fig.show() . . . . The overall sentiment of the Oxford/AstraZeneca vaccine is therefore significantly more negative than average: . # Get z scores of sentiment for each vaccine vax_names = {&#39;Covaxin&#39;: covaxin, &#39;Sinovac&#39;: sinovac, &#39;Sinopharm&#39;: sinopharm, &#39;Moderna&#39;: moderna, &#39;Oxford/AstraZeneca&#39;: oxford, &#39;PfizerBioNTech&#39;: pfizer} sentiment_zscores = pd.DataFrame() for k, v in vax_names.items(): senti = v[&#39;sentiment&#39;].value_counts(normalize=True) senti[&#39;vaccine&#39;] = k sentiment_zscores = sentiment_zscores.append(senti) for col in [&#39;negative&#39;, &#39;neutral&#39;, &#39;positive&#39;]: sentiment_zscores[col+&#39;_zscore&#39;] = (sentiment_zscores[col] - sentiment_zscores[col].mean())/sentiment_zscores[col].std(ddof=0) sentiment_zscores.set_index(&#39;vaccine&#39;, inplace=True) # Plot the results ax = sentiment_zscores.sort_values(&#39;negative_zscore&#39;)[&#39;negative_zscore&#39;].plot.barh(title=&#39;Z scores of negative sentiment&#39;) ax.set_ylabel(&#39;Vaccine&#39;) ax.set_xlabel(&#39;Z score&#39;); . . Further analysis using &#39;smarter&#39; word clouds . The final thing we will do is to generate word clouds to see which words are indicative of each sentiment. The code below is from this notebook, which contains a more detailed explanation of the methodology used to generate &#39;smarter&#39; word clouds. Please go and upvote the original notebook if you find this part useful! . !pip install -q wordninja !pip install -q pyspellchecker from wordcloud import WordCloud, ImageColorGenerator import wordninja from spellchecker import SpellChecker from collections import Counter import matplotlib.pyplot as plt import re import math import random import nltk nltk.download(&#39;wordnet&#39;) nltk.download(&#39;stopwords&#39;) from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords stop_words = set(stopwords.words(&#39;english&#39;)) stop_words.add(&quot;amp&quot;) . . [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Package wordnet is already up-to-date! [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! . # FUNCTIONS REQUIRED def flatten_list(l): return [x for y in l for x in y] def is_acceptable(word: str): return word not in stop_words and len(word) &gt; 2 # Color coding our wordclouds def red_color_func(word, font_size, position, orientation, random_state=None,**kwargs): return f&quot;hsl(0, 100%, {random.randint(25, 75)}%)&quot; def green_color_func(word, font_size, position, orientation, random_state=None,**kwargs): return f&quot;hsl({random.randint(90, 150)}, 100%, 30%)&quot; def yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs): return f&quot;hsl(42, 100%, {random.randint(25, 50)}%)&quot; # Reusable function to generate word clouds def generate_word_clouds(neg_doc, neu_doc, pos_doc): # Display the generated image: fig, axes = plt.subplots(1,3, figsize=(20,10)) wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=&quot;white&quot;).generate(&quot; &quot;.join(neg_doc)) axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation=&#39;bilinear&#39;) axes[0].set_title(&quot;Negative Words&quot;) axes[0].axis(&quot;off&quot;) wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=&quot;white&quot;).generate(&quot; &quot;.join(neu_doc)) axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation=&#39;bilinear&#39;) axes[1].set_title(&quot;Neutral Words&quot;) axes[1].axis(&quot;off&quot;) wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=&quot;white&quot;).generate(&quot; &quot;.join(pos_doc)) axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation=&#39;bilinear&#39;) axes[2].set_title(&quot;Positive Words&quot;) axes[2].axis(&quot;off&quot;) plt.tight_layout() plt.show(); def get_top_percent_words(doc, percent): # Returns a list of &quot;top-n&quot; most frequent words in a list top_n = int(percent * len(set(doc))) counter = Counter(doc).most_common(top_n) top_n_words = [x[0] for x in counter] return top_n_words def clean_document(doc): spell = SpellChecker() lemmatizer = WordNetLemmatizer() # Lemmatize words (needed for calculating frequencies correctly ) doc = [lemmatizer.lemmatize(x) for x in doc] # Get the top 10% of all words. This may include &quot;misspelled&quot; words top_n_words = get_top_percent_words(doc, 0.1) # Get a list of misspelled words misspelled = spell.unknown(doc) # Accept the correctly spelled words and top_n words clean_words = [x for x in doc if x not in misspelled or x in top_n_words] # Try to split the misspelled words to generate good words (ex. &quot;lifeisstrange&quot; -&gt; [&quot;life&quot;, &quot;is&quot;, &quot;strange&quot;]) words_to_split = [x for x in doc if x in misspelled and x not in top_n_words] split_words = flatten_list([wordninja.split(x) for x in words_to_split]) # Some splits may be nonsensical, so reject them (&quot;llouis&quot; -&gt; [&#39;ll&#39;, &#39;ou&#39;, &quot;is&quot;]) clean_words.extend(spell.known(split_words)) return clean_words def get_log_likelihood(doc1, doc2): doc1_counts = Counter(doc1) doc1_freq = { x: doc1_counts[x]/len(doc1) for x in doc1_counts } doc2_counts = Counter(doc2) doc2_freq = { x: doc2_counts[x]/len(doc2) for x in doc2_counts } doc_ratios = { # 1 is added to prevent division by 0 x: math.log((doc1_freq[x] +1 )/(doc2_freq[x]+1)) for x in doc1_freq if x in doc2_freq } top_ratios = Counter(doc_ratios).most_common() top_percent = int(0.1 * len(top_ratios)) return top_ratios[:top_percent] # Function to generate a document based on likelihood values for words def get_scaled_list(log_list): counts = [int(x[1]*100000) for x in log_list] words = [x[0] for x in log_list] cloud = [] for i, word in enumerate(words): cloud.extend([word]*counts[i]) # Shuffle to make it more &quot;real&quot; random.shuffle(cloud) return cloud . . # Convert string to a list of words vax_tweets[&#39;words&#39;] = vax_tweets.text.astype(str).apply(lambda x:re.findall(r&#39; w+&#39;, x )) def get_smart_clouds(df): neg_doc = flatten_list(df[df[&#39;sentiment&#39;]==&#39;negative&#39;][&#39;words&#39;]) neg_doc = [x for x in neg_doc if is_acceptable(x)] pos_doc = flatten_list(df[df[&#39;sentiment&#39;]==&#39;positive&#39;][&#39;words&#39;]) pos_doc = [x for x in pos_doc if is_acceptable(x)] neu_doc = flatten_list(df[df[&#39;sentiment&#39;]==&#39;neutral&#39;][&#39;words&#39;]) neu_doc = [x for x in neu_doc if is_acceptable(x)] # Clean all the documents neg_doc_clean = clean_document(neg_doc) neu_doc_clean = clean_document(neu_doc) pos_doc_clean = clean_document(pos_doc) # Combine classes B and C to compare against A (ex. &quot;positive&quot; vs &quot;non-positive&quot;) top_neg_words = get_log_likelihood(neg_doc_clean, flatten_list([pos_doc_clean, neu_doc_clean])) top_neu_words = get_log_likelihood(neu_doc_clean, flatten_list([pos_doc_clean, neg_doc_clean])) top_pos_words = get_log_likelihood(pos_doc_clean, flatten_list([neu_doc_clean, neg_doc_clean])) # Generate syntetic a corpus using our loglikelihood values neg_doc_final = get_scaled_list(top_neg_words) neu_doc_final = get_scaled_list(top_neu_words) pos_doc_final = get_scaled_list(top_pos_words) # Visualise our synthetic corpus generate_word_clouds(neg_doc_final, neu_doc_final, pos_doc_final) get_smart_clouds(vax_tweets) . . This looks pretty good! The positive tweets appear to be from people who have just received their first vaccine or are grateful for the job scientists and healthcare workers are doing, whereas the negative tweets seem to be from people who have suffered adverse reactions to the vaccine. The neutral tweets seem to be more like news, which could explain why it is the most prevelant sentiment; in fact, the vast majority of tweets contain urls: . vax_tweets[&#39;has_url&#39;] = np.where(vax_tweets[&#39;orig_text&#39;].str.contains(&#39;http&#39;), &#39;yes&#39;, &#39;no&#39;) vax_tweets[&#39;has_url&#39;].value_counts(normalize=True).plot.bar(title=&#39;Does the tweet contain a url?&#39;); . . Interestingly, Canada shows up in the negative word cloud, as well as a couple of Canadian cities. Looking at a &#39;naive&#39; word cloud for tweets containing &#39;Canada&#39; shows us that this appears to be a political/economic issue: . def get_cloud(df, string, c_func): string_l = string.lower() df[string_l] = np.where(df[&#39;text&#39;].str.lower().str.contains(string_l), 1, 0) cloud_df = df.copy()[df[string_l]==1] doc = flatten_list(cloud_df[&#39;words&#39;]) doc = [x for x in doc if is_acceptable(x)] doc = clean_document(doc) fig, axes = plt.subplots(figsize=(9,5)) wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=&quot;white&quot;).generate(&quot; &quot;.join(doc)) axes.imshow(wordcloud.recolor(color_func=c_func, random_state=3), interpolation=&#39;bilinear&#39;) axes.set_title(&quot;Naive word cloud for tweets Containg &#39;%s&#39;&quot; % (string)) axes.axis(&quot;off&quot;) plt.show(); get_cloud(vax_tweets, &#39;Canada&#39;, red_color_func) . . At the time of writing Canada&#39;s vaccination progress has been slower than other developed nations, and people are predicting that it might have an impact on Canada&#39;s economic recovery: . countries = [&#39;Canada&#39;, &#39;United Kingdom&#39;, &#39;United States&#39;, &#39;Chile&#39;, &#39;Singapore&#39;, &#39;Israel&#39;, &#39;Australia&#39;] selected = vax_progress[vax_progress[&#39;country&#39;].isin(countries)] eu[&#39;country&#39;] = &#39;EU median&#39; fig = px.line(vax_progress[vax_progress[&#39;country&#39;].isin(countries)].append(eu), x=&#39;date&#39;, y=&#39;daily_vaccinations_per_million&#39;, color=&#39;country&#39;, title=&#39;Daily vaccinations per million (all vaccines) in Canada vs selected other developed nations&#39;) fig.show() . . . . Conclusion . We were able to gain some interesting insights here, so hopefully you found this useful! That said, there is still a lot left to explore, especially since vaccinations are ongoing and the dataset is still being updated at the time of writing (thanks once again to Gabriel Preda for providing the data). . If you made it this far, I encourage you to give this task a go yourself and see what you can find out! A couple of suggestions: . Try to improve the accuracy of the fastai models we created in part 1. | Instead of looking at each vaccine individually, investigate each vaccination scheme (most countries are using more than one vaccine). | Dig deeper on the sentiment in a specific country and how that relates to vaccination progress. You could even analyse a large dataset of all COVID-19 tweets, not just vaccine specific ones! | Investigate adverse reactions to the vaccine and how that is reflected tweet sentiment. For instance, is blood clotting really a concern for patients who have received the Oxford/AstraZeneca vaccine? | Thanks for reading! . 1. Cover image via https://www.mamamia.com.au/covid-19-vaccine-latest-update/‚Ü© .",
            "url": "https://www.thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/visualisation/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-2.html",
            "relUrl": "/fastai/nlp/sentiment%20analysis/pytorch/visualisation/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-2.html",
            "date": " ‚Ä¢ Mar 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "COVID-19 Vaccine Tweet Sentiment Analysis with fastai - Part 1",
            "content": "Introduction . In this post we will create a model to perform sentiment analysis on tweets about COVID-19 vaccines using the fastai library. I will provide a brief overview of the process here, but a much more in-depth explanation of NLP with fastai can be found in lesson 8 of the fastai course. For convenience clicking on inline code written like this will take you to the relevant part of the fastai documentation where appropriate. In part 2 we will use the model for analysis, looking at changes in tweet sentiment over time and how that relates to the progress of vaccination in different countries. . Transfer learning in NLP - the ULMFiT approach . We will be making use of transfer learning to help us create a model to analyse tweet sentiment. The idea behind transfer learning is that neural networks learn information that generalises to new problems, particularly the early layers of the network. In computer vision, for example, we can take a model that was trained on the ImageNet dataset to recognise different features of images such as circles, then apply that to a smaller dataset and fine-tune the model to be more suited to a specific task (e.g. classifying images as cats or dogs). This technique allows us to train neural networks much faster and with far less data than we would otherwise need. . In 2018 a paper introduced a transfer learning technique for NLP called &#39;Universal Language Model Fine-Tuning&#39; (ULMFiT). The approach is as follows: . Train a language model to predict the next word in a sentence. This step is already done for us; with fastai we can download a model that has been pre-trained for this task on millions of Wikipedia articles. A good language model already knows a lot about how language works in general - for instance, given the sentence &#39;Tokyo is the capital of&#39;, the model might predict &#39;Japan&#39; as the next word. In this case the model understands that Tokyo is closely related to Japan and that &#39;capital&#39; refers to &#39;city&#39; here instead of &#39;upper-case&#39; or &#39;money&#39;. | Fine-tune the language model to a more specific task. The pre-trained language model is good at understanding Wikipedia English, but Twitter English is a bit different. We can take the information the Wikipedia model has learned and apply that to a Twitter dataset to get a Twitter language model that is good at predicting the next word in a tweet. | Fine-tune a classification model to identify sentiment using the pre-trained language model. The idea here is that since our language model already knows a lot about Twitter English, it&#39;s not a huge leap from there to train a classifier that understands that &#39;love&#39; refers to positive sentiment and &#39;hate&#39; refers to negative sentiment. If we tried to train a classifier without using a pre-trained model it would have to learn the whole language from scratch first, which would be very difficult and time consuming. | . This notebook will walk through steps 2 and 3 with fastai. We will then apply the model to unlabelled COVID-19 vaccine tweets and save the results for analysis in part 2. . Important: You will need a GPU to train models with fastai, but fortunately for us Google Colab provides us with access to one for free! To use it, select &#8217;Runtime&#8217; from the menu at the top of the notebook, then &#8217;Change runtime type&#8217;, and ensure your hardware accelerator is set to &#8217;GPU&#8217; before continuing! . Data preparation . This is a write-up of a submission I made for several Kaggle tasks. The tasks are still open and accepting new entries at the time of writing if you want to enter as well! On Kaggle the data is already readily available when using their notebook servers; however, we are using Google Colab today, so we will need to access the Kaggle API to download the data. . Note: Kaggle also have free GPU credits if you prefer to work on their notebook servers instead. . Getting the data from Kaggle . The first step is to create an API token. To do this, the steps are as follows: . Go to &#39;Account&#39; on Kaggle and scroll down to the &#39;API&#39; section. | Expire all current API tokens by clicking &#39;Expire API Token&#39;. | Click &#39;Create New API Token&#39;, which will automatically download a file called kaggle.json. | Upload the kaggle.json file using the file uploader widget below. | # See https://neptune.ai/blog/google-colab-dealing-with-files for more tips on working with files in Colab from google.colab import files uploaded = files.upload() . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . Next, we need to install the Kaggle API. . Note: The API is already preinstalled in Google Colab, but sometimes it&#8217;s an outdated version, so it&#8217;s best to upgrade it in case. . !pip uninstall -q -y kaggle !pip install -q --upgrade pip !pip install -q --upgrade kaggle . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.5MB 5.4MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 2.8 MB/s Building wheel for kaggle (setup.py) ... done . The API docs tell us that we need to ensure kaggle.json is in the location ~/.kaggle/kaggle.json, so let&#39;s make the directory and move the file. . # https://www.machinelearningmindset.com/kaggle-dataset-in-google-colab/ !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ # Check the file in its new directory !ls /root/.kaggle/ # Check the file permission !ls -l ~/.kaggle/kaggle.json #Change the file permission # chmod 600 file ‚Äì owner can read and write # chmod 700 file ‚Äì owner can read, write and execute !chmod 600 ~/.kaggle/kaggle.json . kaggle.json -rw-r--r-- 1 root root 63 Mar 17 15:35 /root/.kaggle/kaggle.json . Now we can download the data using !kaggle dataset download -d username-of-dataset-creator/name-of-dataset. . Note: There is also an API download command on the dataset page that you can copy/paste instead. . # We will be using two datasets for this part, as well as a third dataset for part 2 # To save time in part 2 I&#39;m going to download them all now and save locally !kaggle datasets download -d gpreda/all-covid19-vaccines-tweets !kaggle datasets download -d maxjon/complete-tweet-sentiment-extraction-data !kaggle datasets download -d gpreda/covid-world-vaccination-progress . Downloading all-covid19-vaccines-tweets.zip to /content 0% 0.00/4.76M [00:00&lt;?, ?B/s] 100% 4.76M/4.76M [00:00&lt;00:00, 156MB/s] Downloading complete-tweet-sentiment-extraction-data.zip to /content 0% 0.00/2.58M [00:00&lt;?, ?B/s] 100% 2.58M/2.58M [00:00&lt;00:00, 148MB/s] Downloading covid-world-vaccination-progress.zip to /content 0% 0.00/146k [00:00&lt;?, ?B/s] 100% 146k/146k [00:00&lt;00:00, 61.1MB/s] . The files will be downloaded in .zip format, so let&#39;s unzip them. . # To unzip you can use the following: #!mkdir folder_name #!unzip anyfile.zip -d folder_name # Or unzip all !unzip -q *.zip . 3 archives were successfully processed. . Loading and cleaning the data . As with kaggle, an older version of fastai is preinstalled in Colab, so we will need to upgrade it first. . Important: Make a note of the fastai version you are using, since any models you create and save will need to be run using the same version later. . ! [ -e /content ] &amp;&amp; pip install -Uqq fastai # upgrade fastai on colab import fastai; fastai.__version__ . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 193 kB 4.1 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 776.8 MB 18 kB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.8 MB 23 kB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53 kB 1.4 MB/s ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. torchtext 0.9.0 requires torch==1.8.0, but you have torch 1.7.1 which is incompatible. . &#39;2.2.7&#39; . Let&#39;s import fastai&#39;s text module and take a look at our data. . Tip: If you use import *, useful libraries like pandas and numpy will also be imported at the same time! . from fastai.text.all import * . vax_tweets = pd.read_csv(&#39;vaccination_all_tweets.csv&#39;) vax_tweets[[&#39;date&#39;, &#39;text&#39;, &#39;hashtags&#39;, &#39;user_followers&#39;]].head() . date text hashtags user_followers . 0 2020-12-20 06:06:44 | Same folks said daikon paste could treat a cytokine storm #PfizerBioNTech https://t.co/xeHhIMg1kF | [&#39;PfizerBioNTech&#39;] | 405 | . 1 2020-12-13 16:27:13 | While the world has been on the wrong side of history this year, hopefully, the biggest vaccination effort we&#39;ve ev‚Ä¶ https://t.co/dlCHrZjkhm | NaN | 834 | . 2 2020-12-12 20:33:45 | #coronavirus #SputnikV #AstraZeneca #PfizerBioNTech #Moderna #Covid_19 Russian vaccine is created to last 2-4 years‚Ä¶ https://t.co/ieYlCKBr8P | [&#39;coronavirus&#39;, &#39;SputnikV&#39;, &#39;AstraZeneca&#39;, &#39;PfizerBioNTech&#39;, &#39;Moderna&#39;, &#39;Covid_19&#39;] | 10 | . 3 2020-12-12 20:23:59 | Facts are immutable, Senator, even when you&#39;re not ethically sturdy enough to acknowledge them. (1) You were born i‚Ä¶ https://t.co/jqgV18kch4 | NaN | 49165 | . 4 2020-12-12 20:17:19 | Explain to me again why we need a vaccine @BorisJohnson @MattHancock #whereareallthesickpeople #PfizerBioNTech‚Ä¶ https://t.co/KxbSRoBEHq | [&#39;whereareallthesickpeople&#39;, &#39;PfizerBioNTech&#39;] | 152 | . We could use the text column of this dataset to train a Twitter language model, but since our end goal is sentiment analysis we will need to find another dataset that also contains sentiment labels to train our classifier. Let&#39;s use &#39;Complete Tweet Sentiment Extraction Data&#39;, which contains 40,000 tweets labelled as either negative, neutral or positive sentiment. For more accurate results you could use the &#39;sentiment140&#39; dataset instead, which contains 1.6m tweets labelled as either positive or negative. . tweets = pd.read_csv(&#39;tweet_dataset.csv&#39;) tweets[[&#39;old_text&#39;, &#39;new_sentiment&#39;]].head() . old_text new_sentiment . 0 @tiffanylue i know i was listenin to bad habit earlier and i started freakin at his part =[ | NaN | . 1 Layin n bed with a headache ughhhh...waitin on your call... | negative | . 2 Funeral ceremony...gloomy friday... | negative | . 3 wants to hang out with friends SOON! | positive | . 4 @dannycastillo We want to trade with someone who has Houston tickets, but no one will. | neutral | . For our language model, the only input we need is the tweet text. As we will see in a moment fastai can handle text preprocessing and tokenization for us, but it might be a good idea to remove things like twitter handles, urls, hashtags and emojis first. You could experiment with leaving these in for your own models and see how it affects the results. There are also some rows with blank tweets which need to be removed. . We ideally want the language model to learn not just about tweet language, but more specifically about vaccine tweet language. We can therefore use text from both datasets as input for the language model. For the classification model we need to remove all rows with missing sentiment, however. . def de_emojify(inputString): return inputString.encode(&#39;ascii&#39;, &#39;ignore&#39;).decode(&#39;ascii&#39;) # Code via https://www.kaggle.com/pawanbhandarkar/generate-smarter-word-clouds-with-log-likelihood def tweet_proc(df, text_col=&#39;text&#39;): df[&#39;orig_text&#39;] = df[text_col] # Remove twitter handles df[text_col] = df[text_col].apply(lambda x:re.sub(&#39;@[^ s]+&#39;,&#39;&#39;,x)) # Remove URLs df[text_col] = df[text_col].apply(lambda x:re.sub(r&quot;http S+&quot;, &quot;&quot;, x)) # Remove emojis df[text_col] = df[text_col].apply(de_emojify) # Remove hashtags df[text_col] = df[text_col].apply(lambda x:re.sub(r&#39; B# S+&#39;,&#39;&#39;,x)) return df[df[text_col]!=&#39;&#39;] # Clean the text data and combine the dfs tweets = tweets[[&#39;old_text&#39;, &#39;new_sentiment&#39;]].rename(columns={&#39;old_text&#39;:&#39;text&#39;, &#39;new_sentiment&#39;:&#39;sentiment&#39;}) vax_tweets[&#39;sentiment&#39;] = np.nan tweets = tweet_proc(tweets) vax_tweets = tweet_proc(vax_tweets) df_lm = tweets[[&#39;text&#39;, &#39;sentiment&#39;]].append(vax_tweets[[&#39;text&#39;, &#39;sentiment&#39;]]) df_clas = df_lm.dropna(subset=[&#39;sentiment&#39;]) print(len(df_lm), len(df_clas)) . 70732 31323 . df_clas.head() . text sentiment . 1 Layin n bed with a headache ughhhh...waitin on your call... | negative | . 2 Funeral ceremony...gloomy friday... | negative | . 3 wants to hang out with friends SOON! | positive | . 4 We want to trade with someone who has Houston tickets, but no one will. | neutral | . 5 Re-pinging why didn&#39;t you go to prom? BC my bf didn&#39;t like my friends | negative | . Training a language model . To train our language model we can use self-supervised learning; we just need to give the model some text as an independent variable and fastai will automatically preprocess it and create a dependent variable for us. We can do this in one line of code using the DataLoaders class, which converts our input data into a DataLoader object that can be used as an input to a fastai Learner. . dls_lm = TextDataLoaders.from_df(df_lm, text_col=&#39;text&#39;, is_lm=True, valid_pct=0.1) . /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . Here we told fastai that we are working with text data, which is contained in the text column of a pandas DataFrame called df_lm. We set is_lm=True since we want to train a language model, so fastai needs to label the input data for us. Finally, we told fastai to hold out a random 10% of our data for a validation set using valid_pct=0.1. . Let&#39;s take a look at the first two rows of the DataLoader using show_batch. . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxup rip xxmaj big cup ‚Ä¶ i will miss you xxbos xxmaj morning do n&#39;t ask me why xxmaj i &#39;m up so early xxbos xxmaj swiss drugmaker to help make xxbos xxmaj roast was yummy , i think mum was impressed xxrep 3 ! xxbos tol xxrep 3 d you there was thunder ! ew now it &#39;s all rainy xxup d : i &#39;m scared ! xxbos thanks xxmaj | xxup rip xxmaj big cup ‚Ä¶ i will miss you xxbos xxmaj morning do n&#39;t ask me why xxmaj i &#39;m up so early xxbos xxmaj swiss drugmaker to help make xxbos xxmaj roast was yummy , i think mum was impressed xxrep 3 ! xxbos tol xxrep 3 d you there was thunder ! ew now it &#39;s all rainy xxup d : i &#39;m scared ! xxbos thanks xxmaj xxunk | . 1 prime minister for n xxmaj xxunk xxmaj xxunk . xxbos xxmaj thread : xxmaj as the rolls out the &amp; &amp; how s your state , ( district or xxunk xxbos xxmaj an hour of walking in hot weather = a satisfied but hurting xxmaj xxunk . xxmaj ow , blisters . xxbos awesome that s what i m xxunk the song paranoid . its stuck in my head ! and i | minister for n xxmaj xxunk xxmaj xxunk . xxbos xxmaj thread : xxmaj as the rolls out the &amp; &amp; how s your state , ( district or xxunk xxbos xxmaj an hour of walking in hot weather = a satisfied but hurting xxmaj xxunk . xxmaj ow , blisters . xxbos awesome that s what i m xxunk the song paranoid . its stuck in my head ! and i love | . We have a new column, text_, which is text offset by one. This is the dependent variable fastai created for us. By default fastai uses word tokenization, which splits the text on spaces and punctuation marks and breaks up words like can&#39;t into two separate tokens. fastai also has some special tokens starting with &#39;xx&#39; that are designed to make things easier for the model; for example xxmaj indicates that the next word begins with a capital letter and xxunk represents an unknown word that doesn&#39;t appear in the vocabulary very often. You could experiment with subword tokenization instead, which will split the text on commonly occuring groups of letters instead of spaces. This might help if you wanted to leave hashtags in since they often contain multiple words joined together with no spaces, e.g. #CovidVaccine. The fastai tokenization process is explained in much more detail here for those interested. . Fine-tuning the language model . The next step is to create a language model using language_model_learner. . learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . Here we passed language_model_learner our DataLoaders, dls_lm, and the pre-trained RNN model, AWD_LSTM, which is built into fastai. drop_mult is a multiplier applied to all dropouts in the AWD_LSTM model to reduce overfitting. For example, by default fastai&#39;s AWD_LSTM applies EmbeddingDropout with 10% probability (at the time of writing), but we told fastai that we want to reduce that to 3%. The metrics we want to track are perplexity, which is the exponential of the loss (in this case cross entropy loss), and accuracy, which tells us how often our model predicts the next word correctly. We can also train with fp16 to use less memory and speed up the training process. . We can find a good learning rate for training using lr_find and use that to fit our model. . learn.lr_find() . SuggestedLRs(lr_min=0.04365158379077912, lr_steep=0.02754228748381138) . When we created our Learner the embeddings from the pre-trained AWD_LSTM model were merged with random embeddings added for words that weren&#39;t in the vocabulary. The pre-trained layers were also automatically frozen for us. Using fit_one_cycle with our Learner will train only the new random embeddings (i.e. words that are in our Twitter vocab but not the Wikipedia vocab) in the last layer of the neural network. . learn.fit_one_cycle(1, 3e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.390779 | 4.256468 | 0.257898 | 70.560310 | 03:45 | . After one epoch our language model is predicting the next word in a tweet around 25% of the time - not too bad! We can unfreeze the entire model, find a more suitable learning rate and train for a few more epochs to improve the accuracy further. . learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=0.0002511886414140463, lr_steep=7.585775847473997e-07) . learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.960577 | 4.097842 | 0.279537 | 60.210228 | 04:20 | . 1 | 3.799337 | 4.013978 | 0.290340 | 55.366707 | 04:20 | . 2 | 3.624613 | 4.001914 | 0.295858 | 54.702751 | 04:19 | . 3 | 3.470943 | 4.027544 | 0.295395 | 56.122932 | 04:20 | . After a bit more training we can predict the next word in a tweet around 29% of the time. Let&#39;s test the model out by using it to write some random tweets (in this case it will generate some text following &#39;I love&#39;). . TEXT = &quot;I love&quot; N_WORDS = 30 N_SENTENCES = 2 print(&quot; n&quot;.join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES))) . i love it when your back is full ! i love it , and i liked it ! I &#39;m not talking about that to anyone else . i love this one Although i can see the difference in the way , I &#39;m sure i ca n&#39;t get it . first shot DONE ! . Some interesting results there! Let&#39;s save the model encoder so we can use it to fine-tune our classifier. The encoder is all of the model except for the final layer, which converts activations to probabilities of picking each token in the vocabulary. We want to keep the knowledge the model has learned about tweet language but we won&#39;t be using our classifier to predict the next word in a sentence, so we won&#39;t need the final layer any more. . learn.save_encoder(&#39;finetuned_lm&#39;) . Training a sentiment classifier . To get the DataLoaders for our classifier let&#39;s use the DataBlock API this time, which is more customisable. . dls_clas = DataBlock( blocks = (TextBlock.from_df(&#39;text&#39;, seq_len=dls_lm.seq_len, vocab=dls_lm.vocab), CategoryBlock), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;sentiment&#39;), splitter=RandomSplitter() ).dataloaders(df_clas, bs=64) . /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . To use the API, fastai needs the following: . blocks: TextBlock: Our x variable will be text contained in a pandas DataFrame. We want to use the same sequence length and vocab as the language model DataLoaders so we can make use of our pre-trained model. | CategoryBlock: Our y variable will be a single-label category (negative, neutral or positive sentiment). | . | get_x, get_y: Get data for the model by reading the text and sentiment columns from the DataFrame. | splitter: We will use RandomSplitter() to randomly split the data into a training set (80% by default) and a validation set (20%). | dataloaders: Builds the DataLoaders using the DataBlock template we just defined, the df_clas DataFrame and a batch size of 64. | . We can call show batch as before; this time the dependent variable is sentiment. . dls_clas.show_batch(max_n=2) . text category . 0 xxbos xxup pirate xxup voice : xxrep 3 a xxrep 3 r xxrep 3 g xxrep 3 h xxrep 3 ! i 4got xxup my xxup damn xxup wallet xxup at xxup work xxup xxunk xxrep 3 ! xxup dammit xxrep 3 ! xxup so xxup close xxup yet xxup so xxup far xxrep 3 ! xxup now xxup i m xxup starving xxrep 3 ! | negative | . 1 xxbos xxup ugg xxup want xxup to xxup go xxup to xxup xxunk xxup house xxup but i xxup ca nt xxup finna xxup be xxup bored xxup this xxup weekend xxrep 3 ! xxrep 3 u xxup xxunk xxup wanna xxup spend xxup da xxup nite xxup and xxup go xxup see xxup up xxup and xxup go xxup shopping | neutral | . Initialising the Learner is similar to before, but in this case we want a text_classifier_learner. . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() . Finally, we want to load the encoder from the language model we trained earlier, so our classifier uses pre-trained weights. . learn = learn.load_encoder(&#39;finetuned_lm&#39;) . Fine-tuning the classifier . Now we can train the classifier using discriminative learning rates and gradual unfreezing, which has been found to give better results for this type of model. First let&#39;s freeze all but the last layer: . learn.fit_one_cycle(1, 3e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.828995 | 0.759167 | 0.666028 | 00:54 | . Now freeze all but the last two layers: . learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.733606 | 0.615954 | 0.739144 | 00:56 | . Now all but the last three: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.650302 | 0.566739 | 0.763570 | 01:07 | . Finally, let&#39;s unfreeze the entire model and train a bit more: . learn.unfreeze() learn.fit_one_cycle(3, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.606317 | 0.567328 | 0.767401 | 01:23 | . 1 | 0.558857 | 0.560174 | 0.766762 | 01:24 | . 2 | 0.527293 | 0.562808 | 0.766922 | 01:24 | . learn.save(&#39;classifier&#39;) . Path(&#39;models/classifier.pth&#39;) . Our model correctly predicts sentiment around 77% of the time. We could perhaps do better with a larger dataset as mentioned earlier, or different model hyperparameters. It might be worth experimenting with this yourself to see if you can improve the accuracy. . We can quickly sense check the model by calling predict, which returns the predicted sentiment, the index of the prediction and predicted probabilities for negative, neutral and positive sentiment. . learn.predict(&quot;I love&quot;) . (&#39;positive&#39;, tensor(2), tensor([0.0025, 0.0041, 0.9934])) . learn.predict(&quot;I hate&quot;) . (&#39;negative&#39;, tensor(0), tensor([0.9889, 0.0071, 0.0040])) . Classifying unlabelled tweets . To carry out sentiment analysis on the vaccine tweets, we can add them to the DataLoaders as a test set: . pred_dl = dls_clas.test_dl(vax_tweets[&#39;text&#39;]) . We can then make predictions using get_preds: . preds = learn.get_preds(dl=pred_dl) . Finally, we can save the results for analysis later. . vax_tweets[&#39;sentiment&#39;] = preds[0].argmax(dim=-1) vax_tweets[&#39;sentiment&#39;] = vax_tweets[&#39;sentiment&#39;].map({0:&#39;negative&#39;, 1:&#39;neutral&#39;, 2:&#39;positive&#39;}) # Convert dates vax_tweets[&#39;date&#39;] = pd.to_datetime(vax_tweets[&#39;date&#39;], errors=&#39;coerce&#39;).dt.date # Save to csv vax_tweets.to_csv(&#39;vax_tweets_inc_sentiment.csv&#39;) . Conclusion . fastai make NLP really easy, and we were able to get quite good results with a limited dataset and not a lot of training time by using the ULMFiT approach. To summarise, the steps are: . Fine-tune a language model to predict the next word in a tweet, using a model pre-trained on Wikipedia. | Fine-tune a classification model to predict tweet sentiment using the pre-trained language model. | Apply the classifier to unlabelled tweets to analyse sentiment. | In part 2 we will use our new model for analysis, investigating the overall sentiment of each vaccine, how sentiment changes over time and the relationship between sentiment and vaccination progress in different countries. . I hope you found this useful, and thanks very much to Gabriel Preda for providing the data! . 1. Cover image via https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/‚Ü© .",
            "url": "https://www.thomaswhelan.com/fastai/nlp/sentiment%20analysis/pytorch/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.html",
            "relUrl": "/fastai/nlp/sentiment%20analysis/pytorch/2021/03/17/covid-19-vaccine-tweet-sentiment-analysis-with-fastai-part-1.html",
            "date": " ‚Ä¢ Mar 17, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Evaluating penalty takers using empirical Bayes estimation",
            "content": "Why do we assume every player is equally skilled from the penalty spot? . Last weekend there were two big penalty misses in the Premier league, one by Kevin De Bruyne against Liverpool and the other by Ademola Lookman against West Ham, both of which resulted in dropped points for their respective teams. It&#39;s rare to see two penalties that bad in the same weekend - in fact the last time someone completely missed the target from the spot in the Premier league also cost Manchester City three points against Liverpool! So why was Lookman even taking Fulham&#39;s penalty in the first place, and are City so bad at penalties that Ederson might actually be the best man for the job? . Trying to quantify finishing skill is not a new idea in the football analytics community, but penalties are usually an afterthought in the analysis (if they are considered at all). I&#39;ve seen plenty of posts about expected goals (I&#39;ve even written some), and the typical process of dealing with penalties is to just throw away the data and assume penalty xG = penalties taken * average conversion rate. I have been guilty of that myself, but are we really saying Gabriel Jesus (4/10 penalties scored) is just as good as Yaya Tour√© (15/15)? Is Tour√© the same as Matt Le Tissier (27/27)? There are plenty of &#39;best penalty takers of all time&#39; articles that suggest otherwise (1.16 million results on Google). We can do better. . Here are some typical reasons people give for ignoring finishing skill estimates: . On-ball data is noisy and we are missing key information about defender positions, whether the shot with the player&#39;s strong or weak foot and countless other variables. | The sample size of shots from any given location is small. | Even if we could find some signal in the noise, there probably isn&#39;t much difference between the best finisher and the worst anyway. | Whilst you might be able to get away with that line of thinking in general, it seems odd to apply that logic to penalties, since: . The most important concerns aren&#39;t a factor here; the main differences between one penalty and another are goalkeeper skill, pitch conditions, and (dare I say it!) confidence. We could make this process significantly more complicated by trying to account for those variables, but that might be overthinking things given what we&#39;re trying to accomplish. Unlike open-play shots a rocket into the top corner isn&#39;t necessarily better than a delicate chip down the middle (unless your name is Ademola Lookman!), so we can focus more on the results than the execution. If a club wanted to use this process to find their best penalty taker these variables could be controlled, for example by having a penalty shootout (pressure/competitive element) at the end of training (same goalkeeper/pitch conditions) to build up a large sample size of penalty shots for each player in the squad, so a simple model could be just as useful in practice as a more complex one. | We still won&#39;t have a huge sample size with penalties, but at least the shot location is fixed. | Intuitively we know there is at least some difference, so why don&#39;t we try to investigate further? | Empirical Bayes estimation . There are several methods we could use to create a penalty taking skill estimate, but by far the easiest is &#39;empirical Bayes estimation&#39;. . Important: This analysis is heavily inspired by David Robinson&#8217;s excellent post &#8217;Understanding empirical Bayes estimation (using baseball statistics)&#8217;. Since I prefer Python to R I&#8217;ll be replicating some of the steps he used but I won&#8217;t be going as in-depth, so I highly suggest reading through his post as well if you have time! . As you&#39;ll see in a moment we have data on penalties scored and penalties taken for different players, and we want to estimate penalty conversion rate (scored/taken). Robinson has shown that data in this form can be modelled using a beta distribution. In his words: . &quot;The beta distribution can be understood as representing a probability distribution of probabilities - that is, it represents all the possible values of a probability when we don‚Äôt know what that probability is.&quot; . In our case the probability we are looking to estimate is penalty conversion rate (or &#39;probability of a succesful penalty&#39;). Of course we could just take the actual rate, but that&#39;s not very useful in practice. Say John Smith has scored 3/3 penalties, do we really think he is incapable of missing? It would be preferable to ask &#39;what is our realistic best estimate of Smith&#39;s conversion rate given the fact that he has scored 3/3 so far?&#39; This method is known as &#39;Bayesian inference&#39;; we start with a prior estimate of conversion rate (e.g. the league average distribution) and update our estimate to reflect the new evidence we have on Smith (3/3 penalties scored). . Note: I&#8217;m not going to spend much time explaining Bayes&#8217; Theorem, so if you haven&#8217;t come across it before see here for a primer (or here for the short version). . In standard Bayesian inference we usually decide on the prior distribution ahead of time, but we can approximate this method by estimating the prior distribution from our data instead (hence &#39;empirical estimation&#39;). To obtain our prior we&#39;ll follow Robinson&#39;s method - fit a beta distribution to the data and use that as a starting point for each player or team&#39;s prediction. . Estimate a prior from the data . The main dataset we will be using is from FBRef&#39;s Big 5 European Leagues Stats page. Let&#39;s load it in now. . Note: I am ignoring goalkeeper skill for this analysis, but goalkeeper data is also available at FBRef if you want to repeat the steps to estimate penalty save percentage instead! If you aren&#8217;t familiar with Python I wrote an introductory series that you can check out here. . df = pd.read_csv(&#39;https://raw.githubusercontent.com/twhelan22/blog/master/data/big_five_penalties.csv&#39;, encoding=&#39;utf-8&#39;) df.head() . . name team league season penalties_scored penalties_taken . 0 Matt Le Tissier | Southampton | England | 1992 | 2 | 2 | . 1 Dean Saunders | Aston Villa | England | 1992 | 1 | 2 | . 2 Alan Shearer | Blackburn | England | 1992 | 3 | 3 | . 3 John Sheridan | Sheffield Weds | England | 1992 | 2 | 2 | . 4 Andy Sinton | QPR | England | 1992 | 1 | 2 | . The dataset contains penalty statistics for players in France, Germany, Italy, Spain and England&#39;s top division from the 92/93 season to November 2020. Whilst that sounds like a lot of data, most of the players have only taken a handful of penalties: . players = df.groupby(&#39;name&#39;)[[&#39;penalties_scored&#39;, &#39;penalties_taken&#39;]].sum().reset_index() def conversion_rate(df): df[&#39;conversion_rate&#39;] = df[&#39;penalties_scored&#39;] / df[&#39;penalties_taken&#39;] return conversion_rate(players) players.describe() . . penalties_scored penalties_taken conversion_rate . count 2204.00 | 2204.00 | 2204.00 | . mean 3.90 | 5.02 | 0.71 | . std 6.00 | 7.13 | 0.34 | . min 0.00 | 1.00 | 0.00 | . 25% 1.00 | 1.00 | 0.50 | . 50% 2.00 | 2.00 | 0.83 | . 75% 5.00 | 6.00 | 1.00 | . max 91.00 | 105.00 | 1.00 | . This means that over half of the players in the dataset have a conversion rate of 100% or 0%: . sns.displot(data=players, x=&#39;conversion_rate&#39;, kde=True); . . Clearly those players would add a lot of noise to our estimate, but we don&#39;t want to just throw out a big chunk of data if we can help it. To get around this problem, we can groupby &#39;team&#39; and fit a beta distribution to the resulting team-level data instead. We can then use that prior to get predictions for each player afterwards. . Note: If you have a bigger dataset you could use players to get the prior instead! . That looks much more reasonable already! Let&#39;s filter out teams with fewer than 10 attempts and see what we&#39;re left with. . This is what we&#39;d expect to see intuitively; most of the teams are close to the average, with a few teams having performed significantly better or worse than average. Let&#39;s try to fit a beta distribution to this data: . # Plot the distribution of actual conversion rate (our original data) palette = sns.color_palette() fig, ax = plt.subplots(figsize=(8,5)) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) sns.kdeplot(data=teams_filt[&#39;conversion_rate&#39;]) # Fit a beta distribution to the data to get the alpha0 and beta0 parameters # Note that for this to work 0 &lt; data &lt; 1, which is OK in this case, # since a predicted conversion rate of 0% or 100% isn&#39;t realistic in practice alpha0, beta0, _, _ = ss.beta.fit(teams_filt[&#39;conversion_rate&#39;], floc=0, fscale=1) # Generate a beta distribution using alpha0 and beta0 (the prior distribution) prior_dist = ss.beta.rvs(alpha0, beta0, size=10000) # Plot the random beta distribution we just generated sns.kdeplot(data=prior_dist) # Add legend custom_lines = [Line2D([0], [0], color=palette[0], lw=2.5), Line2D([0], [0], color=palette[1], lw=2.5)] ax.legend(custom_lines, [&#39;Actual conversion rate&#39;, &#39;Random beta distribution: nalpha0={:.2f}, beta0={:.2f}&#39;.format(alpha0, beta0)]); . . That looks good enough for what we are trying to acomplish here! The beta distribution we just generated is defined as: . $X sim Beta( alpha_0, beta_0)$ . So what are $ alpha_0$ and $ beta_0$? In this case $ alpha_0$ = penalties scored and $ beta_0$ = penalties missed. With zero evidence we are going to start with the assumption that every player or team has scored $ alpha_0$ times and missed $ beta_0$ times. Our prior estimate of conversion rate is therefore: . $prior :estimate = frac{starting :goals}{starting :goals :+ :starting :misses} = frac{ alpha_0}{ alpha_0 :+ : beta_0}$ . We are essentially giving everyone a &#39;head start&#39; by assuming they have already taken some penalties and converted them at an average rate, which gets around the issue of small sample sizes. To incorporate the additional evidence we have for a particular player or team, we update their distribution as follows: . $Beta( alpha_0 :+ :goals, beta_0 :+ :misses)$ . Let&#39;s take a look at this in action to get a better understanding of the process. . Visualising the effects of empirical Bayes . # Define a function to get predicted conversion rate using alpha0 and beta0 def predicted_conversion(scored, taken, alpha=alpha0, beta=beta0): return (alpha0 + scored) / (alpha0 + beta0 + taken) prior = predicted_conversion(scored=0, taken=0) print(&#39;Start off with alpha0 = {:.2f} goals and beta0 = {:.2f} misses:&#39;.format(alpha0, beta0)) print(&#39;Prior estimate = {:.2f} / ({:.2f} + {:.2f}) = {:.2%} n&#39;.format(alpha0, alpha0, beta0, prior)) print(&#39;Then every time a penalty is taken, add the results to the starting estimate:&#39;) print(&#39;New estimate = (alpha0 + penalties_scored) / (alpha0 + beta0 + penalties_taken) n&#39;) sns.set(style=&#39;ticks&#39;) def plot_dist(scored, missed): taken = scored + missed new_estimate = predicted_conversion(scored=scored, taken=taken) # Plot the prior distribution fig, ax = plt.subplots(figsize=(9,6)) ax.set(title=&#39;After {:n} taken, {:n} scored: estimate = ({:.2f} + {:n}) / ({:.2f} + {:.2f} + {:n}) = {:.2%}&#39;.format(taken, scored, alpha0, scored, alpha0, beta0, taken, new_estimate), ylim=[0, 8], xlabel=&#39;Predicted penalty conversion rate&#39;) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) sns.kdeplot(data=prior_dist, color=palette[-1], fill=True) plt.axvline(prior, 0, 0.95, color=palette[-1], linestyle=&#39;--&#39;) # Plot the new distribution new_dist = ss.beta.rvs(alpha0+scored, beta0+missed, size=10000) sns.kdeplot(data=new_dist, color=palette[3]) plt.axvline(new_estimate, 0, 0.95, color=palette[3], linestyle=&#39;--&#39;) # Add custom legend and axes custom_lines = [Line2D([0], [0], color=palette[3], lw=2.5), Line2D([0], [0], color=palette[-1], lw=2.5)] ax.legend(custom_lines, [&#39;New estimate: %s&#39; % (str(round(new_estimate*100, 2)))+&#39;%&#39;, &#39;Prior estimate: %s&#39; % (str(round(prior*100, 2)))+&#39;%&#39;], loc=&#39;upper left&#39;) sns.despine(right=True, top=True); plot_dist(1, 0) plot_dist(10, 1) . . Start off with alpha0 = 30.83 goals and beta0 = 8.82 misses: Prior estimate = 30.83 / (30.83 + 8.82) = 77.76% Then every time a penalty is taken, add the results to the starting estimate: New estimate = (alpha0 + penalties_scored) / (alpha0 + beta0 + penalties_taken) . After 1 taken, 1 scored our new estimate is very similar to the prior distribution - we don&#39;t want to make drastic changes after just one penalty! After 11 taken, 10 scored the new distribution is taller and narrower, since we are starting to get more confident in our estimate now we have more information. The new distribution has also shifted to the right, since we think it&#39;s more likely this player is above average. However, even with more evidence this player&#39;s predicted conversion rate of 80.61% is still closer to the average rate than the his actual conversion rate of 90.9%. . The process of pulling estimates back towards the average is sometimes referred to as &#39;shrinkage&#39;, which is illustrated more clearly in the interactive chart below. Teams with a small number of attempts are pulled back towards the prior estimate (the horizontal line) significantly, whereas teams with &gt; 150 attempts have a predicted conversion rate close to their actual conversion rate (i.e. close to the diagonal line). In other words, more evidence = less shrinkage, and vice versa. . teams = teams.assign(predicted_conversion=predicted_conversion(teams[&#39;penalties_scored&#39;], teams[&#39;penalties_taken&#39;])) # Set colour scheme to update automatically for each chart scheme = &#39;inferno&#39; col1 = matplotlib.colors.to_hex(sns.color_palette(scheme)[0]) col2 = matplotlib.colors.to_hex(sns.color_palette(scheme)[-1]) # Create an interactive scatter plot from teams showing predicted and actual conversion rate selection = alt.selection_single(on=&#39;mouseover&#39;); points = alt.Chart(teams).mark_circle(size=50).add_selection( selection ).encode( x=alt.X(&#39;conversion_rate&#39;, scale=alt.Scale(domain=(0.1, 1.1)), axis=alt.Axis(format=&#39;%&#39;, title=&#39;Actual conversion rate&#39;)), y=alt.X(&#39;predicted_conversion&#39;, scale=alt.Scale(domain=(0.685, 0.875)), axis=alt.Axis(format=&#39;%&#39;, title=&#39;Predicted conversion rate&#39;)), color=alt.condition(selection, &#39;penalties_taken:Q&#39;, alt.value(&#39;grey&#39;), legend=alt.Legend(title=&#39;Penalties taken&#39;), scale=alt.Scale(scheme=scheme)), opacity=alt.condition(selection, alt.value(0.8), alt.value(0.1)), tooltip=[&#39;team&#39;, &#39;penalties_scored&#39;, &#39;penalties_taken&#39;, alt.Tooltip(&#39;conversion_rate&#39;, format=&#39;.2%&#39;), alt.Tooltip(&#39;predicted_conversion&#39;, format=&#39;.2%&#39;)] ).interactive().properties( width=500, height=500, title=&#39;As the number of penalties taken increases, predicted rate approaches actual rate&#39; ) # Add horizontal line at y = alpha0 / (alpha0 + beta0) overlayh = pd.DataFrame({&#39;y&#39;: [prior]}) hline = alt.Chart(overlayh).mark_rule(color=col1, strokeWidth=2).encode(y=&#39;y:Q&#39;) overlayhtext = pd.DataFrame({&#39;x&#39;: [0.4], &#39;y&#39;: [prior], &#39;text&#39;: [&#39;Prior: y = alpha0 / (alpha0 + beta0)&#39;]}) htext = alt.Chart(overlayhtext).mark_text(color=col1, fontSize=15, baseline=&#39;bottom&#39;).encode(alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Text(&#39;text&#39;)) # Add diagonal line at y = x overlayd = pd.DataFrame({&#39;x&#39;: [-5, 0.7, 5], &#39;y&#39;: [-5, 0.7, 5], &#39;text&#39;: [&#39;&#39;, &#39;y = x&#39;, &#39;&#39;]}) dline = alt.Chart(overlayd).mark_line(color=col2, strokeWidth=2).encode(x=&#39;x:Q&#39;, y=&#39;y:Q&#39;) dtext = alt.Chart(overlayd).mark_text(color=col2, fontSize=15, angle=285, baseline=&#39;top&#39;).encode(alt.X(&#39;x:Q&#39;), alt.Y(&#39;y:Q&#39;), alt.Text(&#39;text&#39;)) alt.layer(points, hline, htext, dline, dtext ).configure_header( titleFontSize=20, titleFontWeight=&#39;normal&#39; ).configure_axis( labelFontSize=11, titleFontSize=14, titleFontWeight=&#39;normal&#39; ).configure_legend( labelFontSize=11, titleFontSize=12, titleFontWeight=&#39;normal&#39; ) . . Who are the best and worst penalty takers? . As we saw earlier, most players in the dataset have only taken a handful of penalties, and some of the players ranked worst in our new metric were players that happened to miss several times in their first few attempts. It might be more interesting to look at players who have taken at least 15 penalties (i.e. a team&#39;s main penalty taker for multiple seasons) instead. Let&#39;s start with the worst players in the dataset: . name penalties_scored penalties_taken conversion_rate predicted_conversion . 1330 Marek Ham≈°√≠k | 7 | 15 | 46.67% | 69.22% | . 1309 Marco Di Vaio | 13 | 22 | 59.09% | 71.10% | . 1152 Kevin Phillips | 11 | 18 | 61.11% | 72.56% | . 125 Andrea Pirlo | 14 | 22 | 63.64% | 72.72% | . 1771 Roberto Baggio | 12 | 19 | 63.16% | 73.03% | . 2107 Wayne Rooney | 23 | 34 | 67.65% | 73.09% | . 501 Diego | 13 | 20 | 65.00% | 73.48% | . 1163 Klaas-Jan Huntelaar | 11 | 17 | 64.71% | 73.84% | . 1226 Luca Toni | 17 | 25 | 68.00% | 73.98% | . 650 Fernando Torres | 20 | 29 | 68.97% | 74.04% | . Some surprising names there perhaps! Pirlo consistently pops up on &#39;best penalty takers&#39; lists, but even if we look at his overall record on Transfermarkt we get a predicted conversion rate of just 74.5%. Roberto Baggio&#39;s career record, on the other hand, is much better than it looks here (83.2%). . Note: Transfermarkt&#8217;s league coverage isn&#8217;t exhaustive but I haven&#8217;t yet found a more comprehensive resource for penalty stats, so I will be using their &#8217;career&#8217; totals throughout the post to supplement the FBRef data. . What about the best players in the dataset? . name penalties_scored penalties_taken conversion_rate predicted_conversion . 1599 Olivier Monterrubio | 27 | 27 | 100.00% | 86.77% | . 1398 Matt Le Tissier | 24 | 24 | 100.00% | 86.15% | . 1825 Ryad Boudebouz | 20 | 20 | 100.00% | 85.22% | . 74 Alessandro Del Piero | 42 | 46 | 91.30% | 85.03% | . 371 Cristhian Stuani | 17 | 17 | 100.00% | 84.43% | . 609 Fabinho | 17 | 17 | 100.00% | 84.43% | . 1544 Nen√™ | 22 | 23 | 95.65% | 84.33% | . 382 Cristiano Ronaldo | 91 | 105 | 86.67% | 84.23% | . 544 Eden Hazard | 26 | 28 | 92.86% | 84.01% | . 1419 Max Kruse | 15 | 15 | 100.00% | 83.86% | . It&#39;s not surprising to see Le Tissier up there, since he is widely considered to be one of the best penalty takers ever. Rickie Lambert only took a few penalties in the Premiership, but he is at 87.24% over his career in all competitions. If you are following along in notebook form you can investigate different players below; some notable names that weren&#39;t in the dataset include goalkeeper Rog√©rio Ceni (84.11%), Michel Platini (86.36%), Marco Van Basten (86.46%), Davor Suker (88.03%), Ferenc Pusk√°s (88.19%), Ronald Koeman (89.54%) and Alan Shearer (89.86%, more on him later!). The best I&#39;ve found so far is Hugo S√°nchez, who&#39;s 65/65 record puts him at 91.57%! . #@title Search for a player: { run: &quot;auto&quot;, vertical-output: true } text = &#39;Totti&#39; #@param {type:&quot;string&quot;} display(players[players[&#39;name&#39;].str.contains(text)]) # Alternative method (non-Colab Jupyter notebooks) def player_search(): text = widgets.Text() display(text) button = widgets.Button(description=&#39;Search&#39;) display(button) def on_button_clicked(b): return display(players[players[&#39;name&#39;].str.contains(text.value)]) button.on_click(on_button_clicked) . . name penalties_scored penalties_taken conversion_rate predicted_conversion . 684 Francesco Totti | 70 | 85 | 82.35% | 80.89% | . #@title Get predicted conversion rate: { run: &quot;auto&quot;, vertical-output: true } scored = 65 #@param {type:&quot;slider&quot;, min:0, max:150, step:1} missed = 0 #@param {type:&quot;slider&quot;, min:0, max:150, step:1} print(&#39;Scored:&#39;, scored, &#39;Missed:&#39;, missed) print(&#39;Predicted conversion rate: {:.2%}&#39;.format(predicted_conversion(scored, scored + missed))) # Alternative method (non-Colab Jupyter notebooks) def player_slider(): def slider(scored, missed): print(&#39;Scored:&#39;, scored, &#39;Missed:&#39;, missed) print(&#39;Predicted conversion rate: {:.2%}&#39;.format(predicted_conversion(scored, scored + missed))) interact(slider, scored=widgets.IntSlider(min=0, max=150, step=1, value=65), missed=widgets.IntSlider(min=0, max=150, step=1, value=0)) . . Scored: 65 Missed: 0 Predicted conversion rate: 91.57% . You might have noticed that this is a rare top ten list without Messi, and in the past he has even been called out for being &#39;phenomenally bad&#39; at penalties. Let&#39;s check whether that criticism was warranted. . Is Lionel Messi &#39;phenomenally bad&#39; at penalties? . Pretty average, actually. . # Initialise the plot fig, ax = plt.subplots(figsize=(9,6)) ax.set(title=&#39;Lionel Messi, Mr. Average&#39;, ylim=[0, 13], xlabel=&#39;Predicted penalty conversion rate&#39;) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) # Generate a beta distribution representing the range of estimates for an average player sns.kdeplot(data=prior_dist, color=&#39;maroon&#39;, fill=True, bw_adjust=3) plt.axvline(prior, 0, 0.95, c=&#39;maroon&#39;, linestyle=&#39;--&#39;) # Generate a beta distribution representing the range of estimates for Messi # Note - using career numbers from Transfermarkt # https://www.transfermarkt.com/lionel-messi/elfmetertore/spieler/28003 goals = 97 misses = 26 messi_dist = ss.beta.rvs(alpha0+goals, beta0+misses, size=10000) sns.kdeplot(data=messi_dist, color=&#39;mediumblue&#39;, fill=True, bw_adjust=3) est1 = predicted_conversion(goals, goals+misses) plt.axvline(est1, 0, 0.95, c=&#39;mediumblue&#39;) # Add custom legend and axes custom_lines = [Line2D([0], [0], color=&#39;mediumblue&#39;, lw=2.5), Line2D([0], [0], color=&#39;maroon&#39;, lw=2.5, linestyle=&#39;--&#39;)] ax.legend(custom_lines, [&#39;Messi: %s&#39; % (str(round(est1*100, 1)))+&#39;%&#39;, &#39;Prior: %s&#39; % (str(round(prior*100, 1)))+&#39;%&#39;], loc=&#39;upper left&#39;) sns.despine(right=True, top=True); . . The beta distribution for our prior estimate is shown in red, with Messi&#39;s beta distribution in blue. . Tip: Definitely make your charts look worse by using Barcelona colours! . The distribution for Messi is taller and narrower than the prior distribution. This is because we have 123 penalty attempts for Messi, whereas our prior distribution starts off with a lot less, so we are more confident in Messi&#39;s prediction. We can also see that whilst Messi isn&#39;t &#39;phenomenally bad&#39; at penalties, in this case we may have found something he doesn&#39;t excel at. . Are Manchester City bad at penalties? . # Filter to the start of the 16/17 season teams_since_16 = df[df[&#39;season&#39;]&gt;2016].groupby(&#39;team&#39;)[[&#39;penalties_scored&#39;, &#39;penalties_taken&#39;]].sum().reset_index() conversion_rate(teams_since_16) teams_since_16 = teams_since_16.assign(predicted_conversion=predicted_conversion(teams_since_16[&#39;penalties_scored&#39;], teams_since_16[&#39;penalties_taken&#39;])) def percentile(all_scores, score): return round(ss.stats.percentileofscore(all_scores, score), 1) city_pct = percentile(teams_since_16[&#39;penalties_taken&#39;], teams_since_16.set_index(&#39;team&#39;).loc[&#39;Manchester City&#39;, &#39;penalties_taken&#39;]) print(&#39;Manchester City have taken more penalties than &#39;+str(city_pct)+&quot;% of teams nin Europe&#39;s big five leagues since 16/17. However...&quot;) . . Manchester City have taken more penalties than 84.5% of teams in Europe&#39;s big five leagues since 16/17. However... . palette = sns.color_palette() score = teams_since_16.set_index(&#39;team&#39;).loc[&#39;Manchester City&#39;, &#39;predicted_conversion&#39;] worse_or_equal = teams_since_16[teams_since_16[&#39;predicted_conversion&#39;]&lt;=score] worse_teams = len(worse_or_equal) - 1 # Create the main plot fig, ax = plt.subplots(figsize=(10,7)) ax.set(title=&quot;Since Pep Guardiola took charge in 16/17, only %d teams in Europe&#39;s big five leagues nhave a worse predicted penalty conversion rate than Manchester City&quot; % (worse_teams), ylim=[0, 20], xlabel=&#39;Predicted penalty conversion rate&#39;) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) sns.kdeplot(data=teams_since_16[&#39;predicted_conversion&#39;], color=&#39;skyblue&#39;, fill=True) plt.axvline(teams_since_16[&#39;predicted_conversion&#39;].mean(), 0, 20, c=&#39;skyblue&#39;, linestyle=&#39;--&#39;) lineheight = 0.65*teams_since_16[teams_since_16[&#39;team&#39;]==&#39;Manchester City&#39;][&#39;penalties_taken&#39;].values[0]/teams_since_16[&#39;penalties_taken&#39;].max() textheight = 20.5*lineheight plt.axvline(score, 0, lineheight, c=palette[0]) plt.text(score, textheight, &#39;Manchester City&#39;, c=palette[0], ha=&#39;right&#39;) # Add rug - https://seaborn.pydata.org/generated/seaborn.rugplot.html#seaborn.rugplot sns.rugplot(data=teams_since_16[&#39;predicted_conversion&#39;], color=&#39;skyblue&#39;, height=0.035) # Add legend custom_lines = [Line2D([0], [0], color=palette[0], lw=2.5), Line2D([0], [0], color=&#39;skyblue&#39;, lw=2.5, linestyle=&#39;--&#39;), Line2D([0], [0], color=&#39;skyblue&#39;, lw=2.5)] ax.legend(custom_lines, [&#39;Manchester City: %s&#39; % (str(round(score*100, 1))+&#39;%&#39;), &#39;Average Team: %s&#39; % (str(round(teams_since_16[&#39;predicted_conversion&#39;].mean()*100, 1))+&#39;%&#39;), &#39;%d total teams&#39; % (len(teams_since_16))]) # Remove axes sns.despine(right=True, top=True); . . team penalties_scored penalties_taken conversion_rate predicted_conversion . 73 Manchester City | 16 | 25 | 64.00% | 72.44% | . 107 Stoke City | 0 | 3 | 0.00% | 72.29% | . 118 Valladolid | 10 | 17 | 58.82% | 72.08% | . 5 Atalanta | 15 | 24 | 62.50% | 72.00% | . 17 Brest | 2 | 6 | 33.33% | 71.92% | . 87 N√ºrnberg | 2 | 6 | 33.33% | 71.92% | . There&#39;s always going to be some variance involved in a stat like that, but there could be other factors at play as well: . # Data from https://www.transfermarkt.com/manchester-city/elfmeterschuetzen/verein/281 # KDB&#39;s estimate includes his miss against Liverpool man_city_career = {&#39;name&#39;: [&#39;Elano&#39;, &#39;Yaya Tour√©&#39;, &#39;Sergio Ag√ºero&#39;, &#39;Carlos Tevez&#39;, &#39;Robinho&#39;, &#39;Mario Balotelli&#39;, &#39;James Milner&#39;, &#39;Raheem Sterling&#39;, &#39;Kevin De Bruyne&#39;, &#39;Gabriel Jesus&#39;, &#39;ƒ∞lkay G√ºndoƒüan&#39;, &#39;Riyad Mahrez&#39;], &#39;penalties_scored&#39;: [23,15,47,24,12,38,30, 2,7,4,9,10], &#39;penalties_taken&#39;: [26,15,60,34,15,43,35, 4,9,10,10,16], &#39;align&#39;: [&#39;left&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;, &#39;center&#39;, &#39;center&#39;, &#39;right&#39;, &#39;right&#39;, &#39;center&#39;, &#39;right&#39;], &#39;palette&#39;: [0,0,2,0,0,0,0, 2,2,2,1,2]} man_city = pd.DataFrame.from_dict(man_city_career) man_city = man_city.assign(predicted_conversion=predicted_conversion(man_city[&#39;penalties_scored&#39;], man_city[&#39;penalties_taken&#39;])).sort_values(by=&#39;predicted_conversion&#39;, ascending=False) # Create the main plot fig, ax = plt.subplots(figsize=(13,8)) ax.set(title=&#39;Hughes, Mancini and Pellegrini had far better options from the penalty spot than Pep Guardiola&#39;, ylim=[0, 27], xlabel=&#39;Predicted penalty conversion rate&#39;) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) palette = sns.color_palette() sns.kdeplot(data=players[&#39;predicted_conversion&#39;], color=&#39;skyblue&#39;, fill=True, bw_adjust=3) # Add mean plt.axvline(players[&#39;predicted_conversion&#39;].mean(), 0, 27, c=&#39;skyblue&#39;, linestyle=&#39;--&#39;) for index, row in man_city.iterrows(): # Set line height to be proportional to number of penalties taken lineheight = row[&#39;penalties_taken&#39;]/players[&#39;penalties_taken&#39;].max() textheight = 27.5*lineheight plt.axvline(row[&#39;predicted_conversion&#39;], 0, lineheight, c=palette[row[&#39;palette&#39;]]) plt.text(row[&#39;predicted_conversion&#39;], textheight, row[&#39;name&#39;], c=palette[row[&#39;palette&#39;]], ha=row[&#39;align&#39;]) # Add rug sns.rugplot(data=players[&#39;predicted_conversion&#39;], color=&#39;skyblue&#39;, height=0.02) # Add legend custom_lines = [Line2D([0], [0], color=palette[0], lw=2.5), Line2D([0], [0], color=palette[2], lw=2.5), Line2D([0], [0], color=palette[1], lw=2.5), Line2D([0], [0], color=&#39;skyblue&#39;, lw=2.5, linestyle=&#39;--&#39;), Line2D([0], [0], color=&#39;skyblue&#39;, lw=2.5),] ax.legend(custom_lines, [&#39;Mansour era nbefore 16/17&#39;, &#39;Guardiola era&#39;, &#39;Current best&#39;, &#39;Average player&#39;, &#39;%d total players&#39; % (len(players))]) # Remove axes sns.despine(right=True, top=True); . . We can clearly see a big difference here pre/post Pep (with the exception of Carlos Tevez, who mainly took penalties in 09/10 after Elano left). In fact, the wealth of options meant that James Milner was rarely called upon to take penalties for City. Balotelli famously never needs to look at the ball when he takes a penalty - Joe Hart once said that his penalties were almost impossible to stop, which was far greater praise at the time than it would be now (sorry Joe!), and did a lap of honour when he finally saved one in training. To answer our earlier question, not only is Yaya Tour√© significantly better than Gabriel Jesus, they are on opposite ends of the scale! Jesus is great at many aspects of the game, but from the penalty spot he&#39;s one of the worst players I&#39;ve seen with a 70.15% predicted conversion rate. . # Plot the range of estimates for Yaya and Jesus fig, ax = plt.subplots(figsize=(9,6)) ax.set(title=&#39;Yaya Tour√© and Gabriel Jesus are probably not equally skilled at taking penalties&#39;, ylim=[0, 9], xlabel=&#39;Predicted penalty conversion rate&#39;) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) # Yaya goals = 15 misses = 0 yaya_dist = ss.beta.rvs(alpha0+goals, beta0+misses, size=10000) sns.kdeplot(data=yaya_dist, color=palette[0], fill=True, bw_adjust=1.5) est1 = predicted_conversion(goals, goals+misses) plt.axvline(est1, 0, 0.95, c=palette[0], linestyle=&#39;--&#39;) # Jesus goals = 4 misses = 6 jesus_dist = ss.beta.rvs(alpha0+goals, beta0+misses, size=10000) sns.kdeplot(data=jesus_dist, color=&#39;skyblue&#39;, fill=True, bw_adjust=1.5) est2 = predicted_conversion(goals, goals+misses) plt.axvline(est2, 0, 0.95, c=&#39;skyblue&#39;, linestyle=&#39;--&#39;) # Add custom legend and axes custom_lines = [Line2D([0], [0], color=&#39;skyblue&#39;, lw=2.5), Line2D([0], [0], color=palette[0], lw=2.5)] ax.legend(custom_lines, [&#39;Jesus: %s&#39; % (str(round(est2*100, 1)))+&#39;%&#39;, &#39;Tour√©: %s&#39; % (str(round(est1*100, 1)))+&#39;%&#39;], loc=&#39;upper left&#39;) sns.despine(right=True, top=True); . . Aside from his well publicised disagreements with Tour√© it&#39;s hard to blame Pep too much, since it appears his current squad are good at everything but penalties. ƒ∞lkay G√ºndoƒüan&#39;s miss came at a time when City had already missed three of their last five so we can certainly speculate that there would have been a lot of pressure to change things; in fact as I alluded to earlier things got so bad that Pep was seriously considering putting Ederson in charge of spot kicks. De Bruyne&#39;s miss against Liverpool bumped his estimate down to about league average, but he was still predicted to be a bit worse than G√ºndoƒüan before that: . # Plot the range of estimates for KDB before and after his penalty against Liverpool fig, ax = plt.subplots(figsize=(9,6)) ax.set(title=&quot;De Bruyne&#39;s predicted conversion rate decreased after his miss against Liverpool&quot;, ylim=[0, 9], xlabel=&#39;Predicted penalty conversion rate&#39;) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) # Before Liverpool penalty goals = 7 misses = 1 kdb_dist = ss.beta.rvs(alpha0+goals, beta0+misses, size=10000) sns.kdeplot(data=kdb_dist, color=&#39;skyblue&#39;, fill=True, bw_adjust=1.5) est1 = predicted_conversion(goals, goals+misses) plt.axvline(est1, 0, 0.95, c=&#39;skyblue&#39;, linestyle=&#39;--&#39;) # After Liverpool penalty misses = 2 kdb_dist = ss.beta.rvs(alpha0+goals, beta0+misses, size=10000) sns.kdeplot(data=kdb_dist, color=palette[0], bw_adjust=1.5) est2 = predicted_conversion(goals, goals+misses) plt.axvline(est2, 0, 0.95, c=palette[0], linestyle=&#39;--&#39;) # Add custom legend and axes custom_lines = [Line2D([0], [0], color=palette[0], lw=2.5), Line2D([0], [0], color=&#39;skyblue&#39;, lw=2.5)] ax.legend(custom_lines, [&#39;New estimate: %s&#39; % (str(round(est2*100, 1)))+&#39;%&#39;, &#39;Old estimate: %s&#39; % (str(round(est1*100, 1)))+&#39;%&#39;], loc=&#39;upper left&#39;) sns.despine(right=True, top=True); . . ƒ∞lkay G√ºndoƒüan is at the 92.2th percentile of 2204 players in the dataset with a predicted penalty conversion rate of 80.22%! . This analysis suggests G√ºndoƒüan deserves another go, and I&#39;d like to see him given the nod the next time he&#39;s on the pitch. We aren&#39;t talking about a massive difference between the two players, but why not take edges anywhere you can, even if they are small ones? We have to assume Ederson is worse than G√ºndoƒüan as well since we have no additional data, but if Ederson really is City&#39;s best penalty taker then why not use him - it can&#39;t really get much worse at this point! . Why isn&#39;t Aleksandar Mitrovi&#263; taking penalties for Fulham? . Antonin Panenka (83.5% predicted conversion rate) might not have been too pleased with the result of Ademola Lookman&#39;s botched attempt against West Ham last weekend. Lookman was heavily criticised, and at the time I was wondering why Aleksandar Mitroviƒá wasn&#39;t the one taking the penalty. Now I know why: . fulham_career = {&#39;name&#39;: [&#39;Aleksandar Mitroviƒá&#39;, &#39;Tom Cairney&#39;, &#39;Ivan Cavaleiro&#39;, &#39;Ademola Lookman n(before miss)&#39;], &#39;penalties_scored&#39;: [15,2,6,0], &#39;penalties_taken&#39;: [22,3,7,0]} fulham = pd.DataFrame.from_dict(fulham_career) fulham = fulham.assign(predicted_conversion=predicted_conversion(fulham[&#39;penalties_scored&#39;], fulham[&#39;penalties_taken&#39;])).sort_values(by=&#39;predicted_conversion&#39;, ascending=False) fulham[&#39;penalties_missed&#39;] = fulham[&#39;penalties_taken&#39;] - fulham[&#39;penalties_scored&#39;] dist_df = pd.DataFrame() for index, row in fulham.iterrows(): dist = ss.beta.rvs(alpha0+row[&#39;penalties_scored&#39;], beta0+row[&#39;penalties_missed&#39;], size=10000) temp = pd.DataFrame({&#39;predicted_conversion&#39;: dist}) temp[&#39;name&#39;] = row[&#39;name&#39;] dist_df = dist_df.append(temp) fig, ax = plt.subplots(figsize=(10,6)) ax.set(title=&quot;It&#39;s not surprising that Aleksandar Mitroviƒá isn&#39;t taking penalties for Fulham&quot;) ax.xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1)) sns.violinplot(data=dist_df, x=&#39;predicted_conversion&#39;, y=&#39;name&#39;, palette=&#39;light:dimgrey&#39;, inner=&#39;quart&#39;, orient=&#39;h&#39;, ax=ax) plt.xlabel(&#39;Predicted penalty conversion rate&#39;) plt.ylabel(&#39;&#39;) sns.despine(right=True, top=True); . . Aleksandar Mitroviƒá is at the 3.8th percentile of 2204 players in the dataset with a predicted penalty conversion rate of 74.34%! . It&#39;s a good job Mitroviƒá isn&#39;t first choice! Cavaleiro appears to be the best candidate by our estimate, but maybe Lookman showed something in training that set him apart from the rest - the Fulham coaches will have seen him take a lot more penalties than we have. Going for a &#39;Panenka&#39; on your first attempt certainly suggests that you at least rate your own ability from the spot pretty highly, although if we use that logic maybe we should be recommending John Stones as City&#39;s go-to option... . Simulating the Euro 96 semi-final shootout . At this point it seems inevitable to most England fans that every major tournament will end in a penalty shootout loss to Germany in the semis. . . Let&#39;s have a look at one example, the Euro 96 shootout, and see whether that&#39;s really the case. . # https://www.transfermarkt.com/spielbericht/index/spielbericht/935928 eng_career = {&#39;name&#39;: [&#39;Alan Shearer&#39;, &#39;Teddy Sheringham&#39;, &#39;Steve McManaman&#39;, &#39;Paul Ince&#39;, &#39;David Platt&#39;, &#39;Paul Gascoigne&#39;, &#39;Darren Anderton&#39;, &#39;Gareth Southgate&#39;, &#39;Tony Adams&#39;, &#39;Stuart Pearce&#39;, &#39;David Seaman&#39;, &#39;Robbie Fowler&#39;], &#39;penalties_scored&#39;: [65,30,0,0,13, 1,6,0,0,10, 0,32], &#39;penalties_taken&#39;: [67,33,0,0,15, 1,6,0,0,10, 0,34], &#39;notes&#39;: [None, None, None, None, None, None, None, None, None, None, None, &#39;bench, not used&#39;]} ger_career = {&#39;name&#39;: [&#39;Stefan Kuntz&#39;, &#39;Mehmet Scholl&#39;, &#39;Andreas M√∂ller&#39;, &#39;Christian Ziege&#39;, &#39;Dieter Eilts&#39;, &#39;Stefan Reuter&#39;, &#39;Matthias Sammer&#39;, &#39;Markus Babbel&#39;, &#39;Andreas K√∂pke&#39;, &#39;Thomas Strunz&#39;, &#39;Thomas H√§√üler&#39;, &#39;Marco Bode&#39;, &#39;Oliver Bierhoff&#39;], &#39;penalties_scored&#39;: [34,14,9,2,8, 10,0,0,2,4, 26,0,15], &#39;penalties_taken&#39;: [39,17,13,3,8, 15,0,0,2,5, 28,1,20], &#39;notes&#39;: [None, &quot;subbed off 77&#39;&quot;, None, None, None, None, None, None, None, &quot;subbed on 118&#39;&quot;, &quot;subbed on 77&#39;&quot;, &quot;subbed on 110&#39;&quot;, &quot;bench, not used&quot;]} england = pd.DataFrame.from_dict(eng_career) england = england.assign(predicted_conversion=predicted_conversion(england[&#39;penalties_scored&#39;], england[&#39;penalties_taken&#39;])).sort_values(by=&#39;predicted_conversion&#39;, ascending=False) germany = pd.DataFrame.from_dict(ger_career) germany = germany.assign(predicted_conversion=predicted_conversion(germany[&#39;penalties_scored&#39;], germany[&#39;penalties_taken&#39;])).sort_values(by=&#39;predicted_conversion&#39;, ascending=False) england . . name penalties_scored penalties_taken notes predicted_conversion . 0 Alan Shearer | 65 | 67 | None | 89.86% | . 11 Robbie Fowler | 32 | 34 | bench, not used | 85.31% | . 1 Teddy Sheringham | 30 | 33 | None | 83.73% | . 9 Stuart Pearce | 10 | 10 | None | 82.24% | . 6 Darren Anderton | 6 | 6 | None | 80.68% | . 4 David Platt | 13 | 15 | None | 80.20% | . 5 Paul Gascoigne | 1 | 1 | None | 78.31% | . 2 Steve McManaman | 0 | 0 | None | 77.76% | . 3 Paul Ince | 0 | 0 | None | 77.76% | . 7 Gareth Southgate | 0 | 0 | None | 77.76% | . 8 Tony Adams | 0 | 0 | None | 77.76% | . 10 David Seaman | 0 | 0 | None | 77.76% | . It looks like England actually had some good penalty takers available, particularly Shearer who has one of the highest predicted conversion rates we have found so far. Robbie Fowler was still young at the time, but it&#39;s notable that England didn&#39;t choose to bring him on just before the shootout despite having three substitutions remaining, since he was prolific from the spot later in his career. . name penalties_scored penalties_taken notes predicted_conversion . 10 Thomas H√§√üler | 26 | 28 | subbed on 77&#39; | 84.01% | . 0 Stefan Kuntz | 34 | 39 | None | 82.43% | . 4 Dieter Eilts | 8 | 8 | None | 81.49% | . 1 Mehmet Scholl | 14 | 17 | subbed off 77&#39; | 79.14% | . 8 Andreas K√∂pke | 2 | 2 | None | 78.83% | . 9 Thomas Strunz | 4 | 5 | subbed on 118&#39; | 78.01% | . 6 Matthias Sammer | 0 | 0 | None | 77.76% | . 7 Markus Babbel | 0 | 0 | None | 77.76% | . 3 Christian Ziege | 2 | 3 | None | 76.98% | . 12 Oliver Bierhoff | 15 | 20 | bench, not used | 76.83% | . 11 Marco Bode | 0 | 1 | subbed on 110&#39; | 75.85% | . 2 Andreas M√∂ller | 9 | 13 | None | 75.65% | . 5 Stefan Reuter | 10 | 15 | None | 74.71% | . Germany did make some substitutions however, bringing their best penalty taker Thomas H√§√üler on in normal time in addition to Marco Bode and Thomas Strunz in extra time. Whilst Bode (a winger) looks worse than average at penalties he was subbed on for a centre-back in an attempt to win the game in extra-time, and he wasn&#39;t chosen as one of the top six options in the shootout. It&#39;s interesting that Thomas Strunz was brought on to take the second penalty in favour of Oliver Bierhoff (who scored twice in the final), since our estimate is that Bierhoff is below average from the spot. Maybe the coaches were onto something... Captain Andreas M√∂ller scored the winning penalty in sudden-death, but it looks like he was probably a below average penalty taker also. . Note: Just a reminder that I&#8217;m still ignoring goalkeeper skill, but Andreas K√∂pke saved 22/84 penalties in his career, so maybe it&#8217;s harder to score against him than a typical goalie! . # Remove unwanted players fowler = england.copy()[england[&#39;name&#39;]==&#39;Robbie Fowler&#39;] england = england.copy()[england[&#39;name&#39;]!=&#39;Robbie Fowler&#39;] germany = germany.copy()[~germany[&#39;name&#39;].isin([&#39;Mehmet Scholl&#39;, &#39;Oliver Bierhoff&#39;])] # Order the players to get Platt taking the second penalty etc # We only know the first six players for each team, so assume the order for the rest england[&#39;order&#39;] = [1,5,3,7,2,4,8,9,6,10,11] england = england.copy().sort_values(&#39;order&#39;) germany[&#39;order&#39;] = [1,5,7,8,2,9,10,4,11,6,3] germany = germany.copy().sort_values(&#39;order&#39;) . . We can see which team is expected to win the shootout by simulating it using the predicted conversion rates for each player to determine whether they scored or missed their penalty. Here is one simulation: . def shootout(team_a, team_b, printer=False): &quot;&quot;&quot;Function to simulate a penalty shootout. Takes two dataframes as input, one for each team. The dataframe should be sorted in the order used for the shootout, i.e. the first player to take a penalty should be at row 1, the second at row 2 etc. &quot;&quot;&quot; a_score = 0 b_score = 0 a_wins = 0 b_wins = 0 a_index = 0 b_index = 0 # df index should be from 0 to len(df) team_a = team_a.reset_index().drop(&#39;index&#39;, axis=1) team_b = team_b.reset_index().drop(&#39;index&#39;, axis=1) while a_wins == b_wins: # Reset index to 0 if every player has taken a penalty in this round if a_index &gt; len(team_a)-1: a_index = 0 if b_index &gt; len(team_b)-1: b_index = 0 # Get Team A result a_player = team_a.iloc[a_index, 0] a_prob = team_a.iloc[a_index, 4] a_result = np.random.choice([0,1], 1, p=[1-a_prob, a_prob]) a_score += a_result if printer != False: print(a_player+&#39;...scores! (%d-%d)&#39; % (a_score, b_score) if a_result ==1 else a_player+&#39;...misses! (%d-%d)&#39; % (a_score, b_score)) # Get Team B result b_player = team_b.iloc[a_index, 0] b_prob = team_b.iloc[a_index, 4] b_result = np.random.choice([0,1], 1, p=[1-a_prob, a_prob]) b_score += b_result if printer != False: print(b_player+&#39;...scores! (%d-%d)&#39; % (a_score, b_score) if b_result ==1 else b_player+&#39;...misses! (%d-%d)&#39; % (a_score, b_score)) a_index += 1 b_index += 1 # Stop the shootout when scores are no longer equal and at least five penalties each have been taken if (a_index &gt;= 5): if (a_score &gt; b_score): a_wins += 1 if printer != False: print(&#39; nTeam A wins! (%d-%d)&#39; % (a_score, b_score)) elif (a_score &lt; b_score): b_wins += 1 if printer != False: print(&#39; nTeam B wins! (%d-%d)&#39; % (b_score, a_score)) return a_wins, b_wins # Run one penalty shootout _, _ = shootout(england, germany, printer=True) . . Alan Shearer...scores! (1-0) Thomas H√§√üler...misses! (1-0) David Platt...scores! (2-0) Thomas Strunz...scores! (2-1) Stuart Pearce...scores! (3-1) Stefan Reuter...scores! (3-2) Paul Gascoigne...scores! (4-2) Christian Ziege...scores! (4-3) Teddy Sheringham...misses! (4-3) Stefan Kuntz...scores! (4-4) Gareth Southgate...misses! (4-4) Andreas M√∂ller...misses! (4-4) Darren Anderton...scores! (5-4) Dieter Eilts...misses! (5-4) Team A wins! (5-4) . Let&#39;s run this thousands of times and calculate how often England would be expected to advance to the final: . num_sims = 5000 def simulate_shootouts(team_a, team_b, num_sims): a_wins = 0 b_wins = 0 for i in range(num_sims): a, b = shootout(team_a, team_b) a_wins += a b_wins += b return a_wins, b_wins eng_wins, ger_wins = simulate_shootouts(england, germany, num_sims) eng_win_pct = str(round((eng_wins/num_sims)*100, 1)) print(&#39;England win probability = %s&#39; % (eng_win_pct+&#39;%&#39;)) . . England win probability = 51.4% . Pretty close, but a loss wasn&#39;t inevitable at least! Unless you believe in curses, that is... . Conclusion . Thanks for reading! I hope you enjoyed this and learned something useful. We could have made a much more complicated model of penalty taking skill, but as discussed earlier extra complexity wouldn&#39;t necessarily be better here. By using empirical Bayes estimation we were able to get some interesting insights out of a limited dataset in just a few steps. If you want to explore this topic further you could use the same process to estimate penalty save percentage, or you could try to improve on this model. A couple of ways to do that would be to use more data or estimate different priors for different groups of players, e.g. should the prior distribution for a team&#39;s main penalty taker be the same as the prior for someone who was only trusted to take a handful of penalties in their career? Let me know what you find out! . Thanks again to David Robinson, whose work make this possible, and to FBRef and Transfermarkt for the data. .",
            "url": "https://www.thomaswhelan.com/bayes/football/visualisation/2020/11/14/evaluating-penalty-takers-using-empirical-bayes.html",
            "relUrl": "/bayes/football/visualisation/2020/11/14/evaluating-penalty-takers-using-empirical-bayes.html",
            "date": " ‚Ä¢ Nov 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "üë§ About Me",
          "content": "Tom Whelan . tom@thomaswhelan.com ¬∑ @tom_whelan ¬∑ twhelan22 . I created this page to write about various data science and machine learning projects I have worked on. Previously I wrote a series teaching machine learning with Python through a fantasy football lens, which can be found at fantasyfutopia.com. .",
          "url": "https://www.thomaswhelan.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://www.thomaswhelan.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}